{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Forked from https://www.kaggle.com/code/abdurrafae/improved-code-interpretation"]},{"cell_type":"markdown","metadata":{},"source":["**Lewis:** the only changes in this notebook are those needed to run the original one with the new Kaggle evaluation API"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["python版本： 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0]\n","torch版本：2.1.2\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_2364/1834013600.py:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n","  import pkg_resources\n"]},{"name":"stdout","output_type":"stream","text":["CUDA版本： 12.1\n"]}],"source":["import sys\n","print('python版本：',sys.version)\n","\n","import pkg_resources\n","\n","def get_package_version(package_name):\n","    try:\n","        version = pkg_resources.get_distribution(package_name).version\n","        return version\n","    except pkg_resources.DistributionNotFound:\n","        return \"Package not found\"\n","\n","package_name = \"torch\"\n","version = get_package_version(package_name)\n","print(f\"{package_name}版本：{version}\")\n","\n","import torch\n","\n","cuda_version = torch.version.cuda\n","print(\"CUDA版本：\", cuda_version)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:30.874877Z","iopub.status.busy":"2024-06-17T14:14:30.874463Z","iopub.status.idle":"2024-06-17T14:14:30.879196Z","shell.execute_reply":"2024-06-17T14:14:30.878315Z","shell.execute_reply.started":"2024-06-17T14:14:30.874851Z"},"trusted":true},"outputs":[],"source":["## Forked From  https://kaggle.com/code/xiaoz259/pure-rng/notebook\n","\n","\n","# credits:\n","# https://www.kaggle.com/code/olyatsimboy/aimo-openmath-mistral-baseline\n","# https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama\n","# https://www.kaggle.com/code/thedrcat/aimo-mixtral-baseline"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:30.880933Z","iopub.status.busy":"2024-06-17T14:14:30.880566Z","iopub.status.idle":"2024-06-17T14:14:30.897104Z","shell.execute_reply":"2024-06-17T14:14:30.896170Z","shell.execute_reply.started":"2024-06-17T14:14:30.880902Z"},"trusted":true},"outputs":[],"source":["import time\n","\n","NOTEBOOK_START_TIME = time.time()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:30.899592Z","iopub.status.busy":"2024-06-17T14:14:30.899345Z","iopub.status.idle":"2024-06-17T14:14:31.357778Z","shell.execute_reply":"2024-06-17T14:14:31.356805Z","shell.execute_reply.started":"2024-06-17T14:14:30.899570Z"},"trusted":true},"outputs":[],"source":["import aimo\n","\n","env = aimo.make_env()\n","iter_test = env.iter_test()"]},{"cell_type":"markdown","metadata":{},"source":["TO-DO\n","\n","Change temperature as the question goes longer\n","Change temperature based on question lenght"]},{"cell_type":"markdown","metadata":{},"source":["# Zero-shot MMOS-DeepSeekMath-7B with self-consistency and generated code reasoning evaluation\n","\n","Self-consistency is a modification of the standard greedy decoding in reasoning pipelines via sampling several diverse answers followed by aggregation, e.g., most common answer ([SC-CoT paper](https://arxiv.org/pdf/2203.11171.pdf)).\n","\n","In this kernel, we will consider MMOS-DeepSeekMath-7B RL-tuned backbone; in my experiments, this model produces more consistent code reasoning and the code block execution will allow us to decrease arithmetic hallucinations."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:50.725896Z","iopub.status.busy":"2024-06-17T14:14:50.724898Z","iopub.status.idle":"2024-06-17T14:14:50.730451Z","shell.execute_reply":"2024-06-17T14:14:50.729576Z","shell.execute_reply.started":"2024-06-17T14:14:50.725858Z"},"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["DEBUG = False\n","\n","QUANT = False\n","\n","if QUANT:\n","    from transformers import BitsAndBytesConfig\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit = True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_use_double_quant=True,\n","    )\n","\n","USE_PAST_KEY = True"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:31.367285Z","iopub.status.busy":"2024-06-17T14:14:31.366914Z","iopub.status.idle":"2024-06-17T14:14:50.722689Z","shell.execute_reply":"2024-06-17T14:14:50.721800Z","shell.execute_reply.started":"2024-06-17T14:14:31.367253Z"},"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Transformers Version: 4.39.3\n","CPU times: user 203 ms, sys: 19.4 ms, total: 223 ms\n","Wall time: 223 ms\n"]}],"source":["%%time\n","if QUANT:\n","    !pip install -U /kaggle/input/accelerate-wheelwhl/accelerate-0.29.1-py3-none-any.whl -qq\n","    !pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq\n","\n","\n","import torch\n","import gc\n","'''\n","accelerate execution a bit\n","'''\n","torch.backends.cuda.enable_mem_efficient_sdp(False)\n","\n","from transformers import (\n","    AutoModelForCausalLM, \n","    AutoTokenizer, \n","    AutoConfig,\n","    StoppingCriteria,\n","    set_seed\n",")\n","\n","import transformers\n","print(f\"Transformers Version: {transformers.__version__}\")\n","set_seed(42)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:59.705246Z","iopub.status.busy":"2024-06-17T14:14:59.704540Z","iopub.status.idle":"2024-06-17T14:14:59.709463Z","shell.execute_reply":"2024-06-17T14:14:59.708511Z","shell.execute_reply.started":"2024-06-17T14:14:59.705213Z"},"papermill":{"duration":1.224774,"end_time":"2024-02-29T09:36:31.21757","exception":false,"start_time":"2024-02-29T09:36:29.992796","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","PRIVATE = True\n","\n","# df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/test.csv')\n","# df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T16:57:03.930642Z","iopub.status.busy":"2024-05-05T16:57:03.930334Z","iopub.status.idle":"2024-05-05T16:57:03.949882Z","shell.execute_reply":"2024-05-05T16:57:03.949096Z","shell.execute_reply.started":"2024-05-05T16:57:03.930618Z"},"trusted":true},"outputs":[],"source":["# if len(df) < 5:\n","#     df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n","#     PRIVATE = False\n","# df.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:19:17.265368Z","iopub.status.busy":"2024-06-17T14:19:17.264623Z","iopub.status.idle":"2024-06-17T14:19:17.271565Z","shell.execute_reply":"2024-06-17T14:19:17.270717Z","shell.execute_reply.started":"2024-06-17T14:19:17.265336Z"},"trusted":true},"outputs":[],"source":["def naive_parse(answer):\n","    '''\n","    get the first consecutive numbers\n","    '''\n","    out = []\n","    start = False\n","    end = False\n","    for l in reversed(list(answer)):\n","        if l in '0123456789' and not end:\n","            start = True\n","            out.append(l)\n","        else:\n","            if start:\n","                end = True\n","        \n","    out = reversed(out)\n","    return ''.join(out)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T16:57:03.961113Z","iopub.status.busy":"2024-05-05T16:57:03.960834Z","iopub.status.idle":"2024-05-05T16:57:03.978771Z","shell.execute_reply":"2024-05-05T16:57:03.977925Z","shell.execute_reply.started":"2024-05-05T16:57:03.961091Z"},"trusted":true},"outputs":[],"source":["import re\n","import sys\n","import subprocess\n","\n","def return_last_print(output, n):\n","    lines = output.strip().split('\\n')\n","    if lines:\n","        return lines[n]\n","    else:\n","        return \"\"\n","\n","def process_code(code, return_shell_output=False):\n","    \n","    def repl(match):\n","        if \"real\" not in match.group():\n","            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n","        else:\n","            return \"{}{}\".format(match.group()[:-1], ')')\n","    code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n","\n","    if return_shell_output:\n","        code = code.replace('\\n', '\\n    ')\n","            # Add a try...except block\n","        code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n","    \n","    if not return_shell_output:\n","        print(code)\n","    with open('code.py', 'w') as fout:\n","        fout.write(code)\n","    \n","    batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n","    try:\n","        shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n","        return_value = return_last_print(shell_output, -1)\n","        print(shell_output)\n","        if return_shell_output:\n","            if return_value=='FAIL':\n","                CODE_STATUS = False\n","                return_value = return_last_print(shell_output, -2)\n","                if \"not defined\" in return_value:\n","                    return_value+='\\nTry checking the formatting and imports'\n","            else:\n","                CODE_STATUS = True\n","            return return_value, CODE_STATUS  \n","        code_output = round(float(eval(return_value))) % 1000\n","    except Exception as e:\n","        print(e,'shell_output')\n","        code_output = -1\n","    \n","    if return_shell_output:\n","        if code_output==-1:\n","            CODE_STATUS = False\n","        else:\n","            CODE_STATUS = True\n","        return code_output, CODE_STATUS  \n","    \n","    \n","    return code_output\n","\n","\n","def process_text_output(output):\n","    result = output    \n","    try:\n","        result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n","\n","        print('BOXED', result_output)\n","        if not len(result_output):\n","            result_output = naive_parse(result)\n","        else:\n","            result_output = result_output[-1]\n","\n","        print('BOXED FINAL', result_output)\n","        if not len(result_output):\n","            result_output = -1\n","        \n","        else:\n","            result_output = round(float(eval(result_output))) % 1000\n","    \n","    except Exception as e:\n","        print(e)\n","        print('ERROR PARSING TEXT')\n","        result_output = -1\n","    \n","    return result_output\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T16:57:03.979978Z","iopub.status.busy":"2024-05-05T16:57:03.9797Z","iopub.status.idle":"2024-05-05T16:57:04.220476Z","shell.execute_reply":"2024-05-05T16:57:04.219456Z","shell.execute_reply.started":"2024-05-05T16:57:03.97995Z"},"trusted":true},"outputs":[{"data":{"text/plain":["40"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T16:57:04.223341Z","iopub.status.busy":"2024-05-05T16:57:04.221614Z","iopub.status.idle":"2024-05-05T16:57:04.229038Z","shell.execute_reply":"2024-05-05T16:57:04.22822Z","shell.execute_reply.started":"2024-05-05T16:57:04.223316Z"},"trusted":true},"outputs":[],"source":["import re\n","import math\n","import random\n","\n","from collections import defaultdict\n","\n","n_repetitions = 17 if PRIVATE else 4 # Original notebook had 22 but times out :(\n","TOTAL_TOKENS = 2048 # if PRIVATE else 512\n","\n","if PRIVATE:\n","    TIME_LIMIT = 31500\n","else:\n","    TIME_LIMIT = 1"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T06:53:23.248311Z","iopub.status.busy":"2024-05-04T06:53:23.248014Z","iopub.status.idle":"2024-05-04T06:56:25.02544Z","shell.execute_reply":"2024-05-04T06:56:25.024594Z","shell.execute_reply.started":"2024-05-04T06:53:23.248286Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ecebded621bd47d39079a3d32782a7df","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#list number of cuda\n","n_gpus = torch.cuda.device_count()\n","device_i = 0\n","if n_gpus > 1:\n","    device_i = 1\n","\n","\n","\n","if PRIVATE:\n","\n","    MODEL_PATH = \"./input/deepseek-math\"#\"/kaggle/input/gemma/transformers/7b-it/1\"\n","    DEEP = True\n","\n","    config = AutoConfig.from_pretrained(MODEL_PATH)\n","    config.gradient_checkpointing = True\n","\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","\n","    device_map = [('model.embed_tokens', 0),\n","                 ('model.layers.0', 0),\n","                 ('model.layers.1', 0),\n","                 ('model.layers.2', 0),\n","                 ('model.layers.3', 0),\n","                 ('model.layers.4', 0),\n","                 ('model.layers.5', 0),\n","                 ('model.layers.6', 0),\n","                 ('model.layers.7', 0),\n","                 ('model.layers.8', 0),\n","                 ('model.layers.9', 0),\n","                 ('model.layers.10', 0),\n","                 ('model.layers.11', 0),\n","                 ('model.layers.12', 0),\n","                 ('model.layers.13', 0),\n","                 ('model.layers.14', 0),\n","                 ('model.layers.15', 0),\n","                 ('model.layers.16', 0),\n","                 ('model.layers.17', 0),\n","                 ('model.layers.18', 0),\n","                 ('model.layers.19', 0),\n","                 ('model.layers.20', 0),\n","                 ('model.layers.21', 0),\n","                 ('model.layers.22', device_i),\n","                 ('model.layers.23', device_i),\n","                 ('model.layers.24', device_i),\n","                 ('model.layers.25', device_i),\n","                 ('model.layers.26', device_i),\n","                 ('model.layers.27', device_i),\n","                 ('model.layers.28', device_i),\n","                 ('model.layers.29', device_i),\n","                 ('model.norm', device_i),\n","                 ('lm_head', device_i)]\n","\n","    device_map = {ii:jj for (ii,jj) in device_map}\n","\n","    if QUANT:\n","        from transformers import BitsAndBytesConfig\n","        quantization_config = BitsAndBytesConfig(\n","            load_in_4bit = True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.bfloat16,\n","            bnb_4bit_use_double_quant=True,\n","        )\n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_PATH,\n","            device_map=\"sequential\",\n","            torch_dtype=\"auto\",\n","            trust_remote_code=True, \n","            quantization_config=quantization_config,\n","            config=config\n","        )\n","    else:  \n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_PATH,\n","            device_map=device_map,\n","            torch_dtype=\"auto\",\n","            trust_remote_code=True,\n","            #quantization_config=quantization_config,\n","            config=config\n","        )\n","    \n","    pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype='auto',\n","    device_map=device_map,\n",")\n","    from transformers import StoppingCriteriaList\n","\n","    class StoppingCriteriaSub(StoppingCriteria):\n","        def __init__(self, stops = [], encounters=1):\n","            super().__init__()\n","            self.stops = [stop.to(\"cuda\") for stop in stops]\n","\n","        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n","            for stop in self.stops:\n","                last_token = input_ids[0][-len(stop):]\n","                if torch.all(torch.eq(stop,last_token)):\n","                    return True\n","            return False\n","\n","\n","    stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"] #,  \n","    stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n","    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n","    \n","    model.dtype, model.hf_device_map"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T06:56:25.029669Z","iopub.status.busy":"2024-05-04T06:56:25.029303Z","iopub.status.idle":"2024-05-04T06:56:25.035377Z","shell.execute_reply":"2024-05-04T06:56:25.034298Z","shell.execute_reply.started":"2024-05-04T06:56:25.029643Z"},"trusted":true},"outputs":[],"source":["code = \"\"\"Below is a math problem you are to solve (positive numerical answer):\n","\\\"{}\\\"\n","To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\n","Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n","\n","Approach:\"\"\"\n","\n","\n","cot = \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n","\\\"{}\\\"\n","Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n","\n","promplt_options = [code,cot]"]},{"cell_type":"code","execution_count":15,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-05-04T06:56:25.037129Z","iopub.status.busy":"2024-05-04T06:56:25.036853Z","iopub.status.idle":"2024-05-04T07:00:32.65925Z","shell.execute_reply":"2024-05-04T07:00:32.658351Z","shell.execute_reply.started":"2024-05-04T06:56:25.037105Z"},"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/10 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n","Solving problem 0 ...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","  File \"/home/dijkstraz/AIMO/code.py\", line 89\n","    distance_AB = x2_sub - x1\n","IndentationError: unexpected indent\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"," 10%|█         | 1/10 [08:53<1:20:02, 533.59s/it]"]},{"name":"stdout","output_type":"stream","text":["Solving problem 1 ...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"," 20%|██        | 2/10 [11:41<42:29, 318.67s/it]  "]},{"name":"stdout","output_type":"stream","text":["Solving problem 2 ...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","  File \"/home/dijkstraz/AIMO/code.py\", line 10\n","    return special numbers\n","                   ^^^^^^^\n","SyntaxError: invalid syntax\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","  File \"/home/dijkstraz/AIMO/code.py\", line 10\n","    return special numbers\n","                   ^^^^^^^\n","SyntaxError: invalid syntax\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"," 30%|███       | 3/10 [14:34<29:24, 252.12s/it]"]},{"name":"stdout","output_type":"stream","text":["Solving problem 3 ...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"," 40%|████      | 4/10 [19:39<27:16, 272.82s/it]"]},{"name":"stdout","output_type":"stream","text":["Solving problem 4 ...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"," 50%|█████     | 5/10 [23:11<20:53, 250.78s/it]"]},{"name":"stdout","output_type":"stream","text":["Solving problem 5 ...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"," 60%|██████    | 6/10 [25:41<14:26, 216.67s/it]"]},{"name":"stdout","output_type":"stream","text":["Solving problem 6 ...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"," 70%|███████   | 7/10 [28:38<10:10, 203.61s/it]"]},{"name":"stdout","output_type":"stream","text":["Solving problem 7 ...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"," 80%|████████  | 8/10 [30:18<05:41, 170.60s/it]"]},{"name":"stdout","output_type":"stream","text":["Solving problem 8 ...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"," 90%|█████████ | 9/10 [41:24<05:25, 325.66s/it]"]},{"name":"stdout","output_type":"stream","text":["Solving problem 9 ...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","100%|██████████| 10/10 [43:47<00:00, 262.74s/it]\n"]}],"source":["import re\n","from collections import defaultdict\n","from collections import Counter\n","\n","from numpy.random import choice\n","import numpy as np\n","\n","tool_instruction = '\\n\\nPlease integrate natural language reasoning with programs to solve the above problem, and put your final numerical answer within \\\\boxed{}.\\nNote that the intermediary calculations may be real numbers, but the final numercal answer would always be an integer.'\n","\n","\n","#tool_instruction = \" The answer should be given as a non-negative modulo 1000.\"\n","#tool_instruction += '\\nPlease integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.'\n","\n","temperature = 0.9\n","top_p = 1.0\n","\n","temperature_coding = 0.9\n","top_p_coding = 1.0\n","\n","   \n","total_results = {}\n","total_answers = {}\n","best_stats = {}\n","total_outputs = {}\n","question_type_counts = {}\n","starting_counts = (2,3)\n","    \n","from IPython.utils import io\n","# LEWIS: I had to invert the loop order because the new API forbids repeated calls on the same problem\n","for i, (test, sample_submission) in tqdm(enumerate(iter_test), total=10):\n","    print(f\"Solving problem {i} ...\")\n","    TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n","\n","    if TIME_SPENT>TIME_LIMIT:\n","        sample_submission['answer'] = 0\n","        env.predict(sample_submission)\n","        break\n","    \n","    with io.capture_output() as captured: # i capture the printouts\n","        \n","        for jj in tqdm(range(n_repetitions)):   \n","    #     for i, (test, sample_submission) in tqdm(enumerate(iter_test)):\n","            \n","\n","    #         id_ = df['id'].loc[i]\n","    #         problem = df['problem'].loc[i]\n","            problem = test['problem'].values[0]\n","            print(f\"\\n\\n\\nQUESTION {i} - {jj} - TIME_SPENT : {TIME_SPENT:.0f} secs\")\n","            \n","            best, best_count = best_stats.get(i,(-1,-1))\n","            if best_count>np.sqrt(jj):\n","                print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n","                continue\n","                \n","            outputs = total_outputs.get(i,[])\n","            text_answers, code_answers = question_type_counts.get(i,starting_counts)\n","            results = total_results.get(i,[])\n","            answers = total_answers.get(i,[])\n","            \n","            for _ in range(5):\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                time.sleep(0.2)\n","\n","            try:\n","                ALREADY_GEN = 0\n","                code_error = None\n","                code_error_count = 0\n","                code_output = -1\n","                #initail_message = problem  + tool_instruction \n","                counts = np.array([text_answers,code_answers])\n","\n","                draw = choice(promplt_options, 1,\n","                            p=counts/counts.sum())\n","\n","                initail_message = draw[0].format(problem,\"{}\")            \n","                prompt = f\"User: {initail_message}\"\n","\n","                current_printed = len(prompt)\n","                print(f\"{jj}_{prompt}\\n\")\n","\n","                model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","                input_len = len(model_inputs['input_ids'][0])\n","\n","                generation_output = model.generate(**model_inputs, \n","                                                max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n","                                                return_dict_in_generate=USE_PAST_KEY,\n","                                                do_sample = True,\n","                                                temperature = temperature,\n","                                                top_p = top_p,\n","                                                num_return_sequences=1, stopping_criteria = stopping_criteria)\n","\n","                if USE_PAST_KEY:\n","                    output_ids = generation_output.sequences[0]\n","                else:\n","                    output_ids = generation_output[0]\n","                decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","                print(f\"{decoded_output[current_printed:]}\\n\")\n","                current_printed += len(decoded_output[current_printed:])\n","                cummulative_code = \"\"\n","                \n","                \n","                stop_word_cond = False\n","                for stop_word in stop_words:\n","                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","                    \n","                \n","                while (stop_word_cond) and (ALREADY_GEN<(TOTAL_TOKENS)):\n","\n","                    if (decoded_output[-len(\"```python\"):]==\"```python\"):\n","                        temperature_inner=temperature_coding\n","                        top_p_inner = top_p_coding\n","                        prompt = decoded_output\n","                    else:\n","                        temperature_inner=temperature\n","                        top_p_inner = top_p\n","                        try:\n","                            if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n","                                code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n","                            else:\n","                                code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n","                            \n","\n","                            cummulative_code+=code_text\n","                            code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n","                            print('CODE RESULTS', code_output)\n","\n","                            if code_error==code_output:\n","                                code_error_count+=1\n","                            else:\n","                                code_error=code_output\n","                                code_error_count = 0\n","\n","                            if not CODE_STATUS:\n","                                cummulative_code = cummulative_code[:-len(code_text)]\n","\n","                                if code_error_count>=1:\n","                                    print(\"REPEATED ERRORS\")\n","                                    break\n","\n","                        except Exception as e:\n","                            print(e)\n","                            print('ERROR PARSING CODE')\n","                            code_output = -1\n","\n","                        if code_output!=-1:\n","                            if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n","                                prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'\n","                            else:\n","                                prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n'\n","                        else:\n","                            prompt = decoded_output\n","                            cummulative_code=\"\"\n","\n","\n","                    model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","                    ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n","\n","                    if USE_PAST_KEY:\n","                        old_values = generation_output.past_key_values\n","                    else:\n","                        old_values = None\n","\n","                    generation_output = model.generate(**model_inputs, \n","                                                    max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n","                                                    return_dict_in_generate=USE_PAST_KEY,\n","                                                    past_key_values=old_values,\n","                                                    do_sample = True,\n","                                                    temperature = temperature_inner,\n","                                                    top_p = top_p_inner,\n","                                                    num_return_sequences=1, stopping_criteria = stopping_criteria)\n","\n","                    if USE_PAST_KEY:\n","                        output_ids = generation_output.sequences[0]\n","                    else:\n","                        output_ids = generation_output[0]\n","                    decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","                    print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n","                    current_printed+=len(decoded_output[current_printed:])\n","                    \n","                    stop_word_cond = False\n","                    for stop_word in stop_words:\n","                        stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","\n","                if USE_PAST_KEY:\n","                    output_ids = generation_output.sequences[0]\n","                else:\n","                    output_ids = generation_output[0]\n","\n","                raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n","                #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n","                result_output = process_text_output(raw_output)\n","                \n","                try:\n","                    code_output = round(float(eval(code_output))) % 1000\n","                except Exception as e:\n","                    print(e,'final_eval')\n","                    code_output = -1\n","\n","            except Exception as e:\n","                print(e,\"5\")\n","                result_output, code_output = -1, -1\n","\n","            if code_output!=-1:\n","                outputs.append(code_output)\n","                code_answers+=1\n","\n","            if result_output!=-1:\n","                outputs.append(result_output)\n","                text_answers+=1\n","\n","            if len(outputs) > 0:\n","                occurances = Counter(outputs).most_common()\n","                print(occurances)\n","                if occurances[0][1] > best_count:\n","                    print(\"GOOD ANSWER UPDATED!\")\n","                    best = occurances[0][0]\n","                    best_count = occurances[0][1]\n","                if occurances[0][1] > 5:\n","                    print(\"ANSWER FOUND!\")\n","                    break\n","\n","            results.append(result_output)\n","            answers.append(code_output)\n","            \n","            best_stats[i] = (best, best_count) \n","            question_type_counts[i] = (text_answers, code_answers)\n","            total_outputs[i] = outputs\n","            \n","            total_results[i] = results\n","            total_answers[i] = answers\n","\n","            print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n","            if DEBUG:\n","                break\n","                \n","        print(f\"Predicted best answer: {best_stats}\")\n","        sample_submission['answer'] = best_stats[i][0]\n","        env.predict(sample_submission)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.660666Z","iopub.status.busy":"2024-05-04T07:00:32.660394Z","iopub.status.idle":"2024-05-04T07:00:32.664955Z","shell.execute_reply":"2024-05-04T07:00:32.663929Z","shell.execute_reply.started":"2024-05-04T07:00:32.660641Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PRIVATE:\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdf\u001b[49m)):\n\u001b[1;32m      4\u001b[0m         a \u001b[38;5;241m=\u001b[39m total_answers[ii]\n\u001b[1;32m      5\u001b[0m         b \u001b[38;5;241m=\u001b[39m total_answers[ii]\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["import numpy as np\n","if PRIVATE:\n","    for ii in range(len(df)):\n","        a = total_answers[ii]\n","        b = total_answers[ii]\n","        a = np.array(a)\n","        b = np.array(b)\n","        print(a,b)\n","        a[a < 0] = b[a < 0]\n","\n","        pred = Counter(a.tolist()).most_common(2)\n","        print(pred)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["with open('temp.md', 'w') as f:\n","    f.write(captured.stdout)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.666603Z","iopub.status.busy":"2024-05-04T07:00:32.66628Z","iopub.status.idle":"2024-05-04T07:00:32.680081Z","shell.execute_reply":"2024-05-04T07:00:32.6793Z","shell.execute_reply.started":"2024-05-04T07:00:32.666572Z"},"trusted":true},"outputs":[],"source":["# if PRIVATE:\n","#     df['answer'] = [best_stats[ii][0] for ii in range(len(df))]\n","# else:\n","#     df['answer'] = 2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.681659Z","iopub.status.busy":"2024-05-04T07:00:32.681425Z","iopub.status.idle":"2024-05-04T07:00:32.690243Z","shell.execute_reply":"2024-05-04T07:00:32.689341Z","shell.execute_reply.started":"2024-05-04T07:00:32.681638Z"},"papermill":{"duration":0.021128,"end_time":"2024-02-29T09:37:05.574782","exception":false,"start_time":"2024-02-29T09:37:05.553654","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# df[['id','answer']].to_csv(\"foo.csv\", header=True, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.692931Z","iopub.status.busy":"2024-05-04T07:00:32.6913Z","iopub.status.idle":"2024-05-04T07:00:32.701164Z","shell.execute_reply":"2024-05-04T07:00:32.700328Z","shell.execute_reply.started":"2024-05-04T07:00:32.692898Z"},"trusted":true},"outputs":[],"source":["# if not PRIVATE:\n","#     df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n","#     if PRIVATE:\n","#         df['model_answer'] = [best_stats[ii][0] for ii in range(len(df))]\n","#         df['match'] = df.answer == df.model_answer\n","#         print(f'{df.match.sum()} matches in {len(df)} examples')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.702653Z","iopub.status.busy":"2024-05-04T07:00:32.702324Z","iopub.status.idle":"2024-05-04T07:00:32.819166Z","shell.execute_reply":"2024-05-04T07:00:32.818269Z","shell.execute_reply.started":"2024-05-04T07:00:32.702621Z"},"trusted":true},"outputs":[],"source":["with open('code.py', 'w') as fout:\n","    fout.write(\"print('done')\")\n","\n","batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n","try:\n","    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n","    print(shell_output)\n","except:\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4281572,"sourceId":7369493,"sourceType":"datasetVersion"},{"datasetId":4720595,"sourceId":8012825,"sourceType":"datasetVersion"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4748944,"sourceId":8052555,"sourceType":"datasetVersion"},{"modelInstanceId":8332,"sourceId":11261,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":11264,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
