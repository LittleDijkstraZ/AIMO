{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Forked from https://www.kaggle.com/code/abdurrafae/improved-code-interpretation"]},{"cell_type":"markdown","metadata":{},"source":["**Lewis:** the only changes in this notebook are those needed to run the original one with the new Kaggle evaluation API"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["python版本： 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0]\n","torch版本：2.1.2\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_8067/3712959345.py:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n","  import pkg_resources\n"]},{"name":"stdout","output_type":"stream","text":["CUDA版本： 12.1\n"]}],"source":["import sys\n","print('python版本：',sys.version)\n","\n","import pkg_resources\n","\n","def get_package_version(package_name):\n","    try:\n","        version = pkg_resources.get_distribution(package_name).version\n","        return version\n","    except pkg_resources.DistributionNotFound:\n","        return \"Package not found\"\n","package_name = \"torch\"\n","version = get_package_version(package_name)\n","print(f\"{package_name}版本：{version}\")\n","\n","import torch\n","\n","cuda_version = torch.version.cuda\n","print(\"CUDA版本：\", cuda_version)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:30.874877Z","iopub.status.busy":"2024-06-17T14:14:30.874463Z","iopub.status.idle":"2024-06-17T14:14:30.879196Z","shell.execute_reply":"2024-06-17T14:14:30.878315Z","shell.execute_reply.started":"2024-06-17T14:14:30.874851Z"},"trusted":true},"outputs":[],"source":["## Forked From  https://kaggle.com/code/xiaoz259/pure-rng/notebook\n","\n","\n","# credits:\n","# https://www.kaggle.com/code/olyatsimboy/aimo-openmath-mistral-baseline\n","# https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama\n","# https://www.kaggle.com/code/thedrcat/aimo-mixtral-baseline"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:30.880933Z","iopub.status.busy":"2024-06-17T14:14:30.880566Z","iopub.status.idle":"2024-06-17T14:14:30.897104Z","shell.execute_reply":"2024-06-17T14:14:30.896170Z","shell.execute_reply.started":"2024-06-17T14:14:30.880902Z"},"trusted":true},"outputs":[],"source":["import time\n","\n","NOTEBOOK_START_TIME = time.time()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:30.899592Z","iopub.status.busy":"2024-06-17T14:14:30.899345Z","iopub.status.idle":"2024-06-17T14:14:31.357778Z","shell.execute_reply":"2024-06-17T14:14:31.356805Z","shell.execute_reply.started":"2024-06-17T14:14:30.899570Z"},"trusted":true},"outputs":[],"source":["import aimo\n","try:\n","    env = aimo.make_env()\n","except:\n","    pass\n","iter_test = env.iter_test()"]},{"cell_type":"markdown","metadata":{},"source":["TO-DO\n","\n","Change temperature as the question goes longer\n","Change temperature based on question lenght"]},{"cell_type":"markdown","metadata":{},"source":["# Zero-shot MMOS-DeepSeekMath-7B with self-consistency and generated code reasoning evaluation\n","\n","Self-consistency is a modification of the standard greedy decoding in reasoning pipelines via sampling several diverse answers followed by aggregation, e.g., most common answer ([SC-CoT paper](https://arxiv.org/pdf/2203.11171.pdf)).\n","\n","In this kernel, we will consider MMOS-DeepSeekMath-7B RL-tuned backbone; in my experiments, this model produces more consistent code reasoning and the code block execution will allow us to decrease arithmetic hallucinations."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:50.725896Z","iopub.status.busy":"2024-06-17T14:14:50.724898Z","iopub.status.idle":"2024-06-17T14:14:50.730451Z","shell.execute_reply":"2024-06-17T14:14:50.729576Z","shell.execute_reply.started":"2024-06-17T14:14:50.725858Z"},"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["DEBUG = False\n","\n","QUANT = False\n","\n","if QUANT:\n","    from transformers import BitsAndBytesConfig\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit = True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_use_double_quant=True,\n","    )\n","\n","USE_PAST_KEY = True"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:31.367285Z","iopub.status.busy":"2024-06-17T14:14:31.366914Z","iopub.status.idle":"2024-06-17T14:14:50.722689Z","shell.execute_reply":"2024-06-17T14:14:50.721800Z","shell.execute_reply.started":"2024-06-17T14:14:31.367253Z"},"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Transformers Version: 4.39.3\n","CPU times: user 223 ms, sys: 9.45 ms, total: 233 ms\n","Wall time: 233 ms\n"]}],"source":["%%time\n","if QUANT:\n","    !pip install -U ./input/accelerate-wheelwhl/accelerate-0.29.1-py3-none-any.whl -qq\n","    !pip install -U ./input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq\n","\n","\n","import torch\n","import gc\n","'''\n","accelerate execution a bit\n","'''\n","torch.backends.cuda.enable_mem_efficient_sdp(False)\n","\n","from transformers import (\n","    AutoModelForCausalLM, \n","    AutoTokenizer, \n","    AutoConfig,\n","    StoppingCriteria,\n","    set_seed\n",")\n","\n","import transformers\n","print(f\"Transformers Version: {transformers.__version__}\")\n","set_seed(42)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:59.705246Z","iopub.status.busy":"2024-06-17T14:14:59.704540Z","iopub.status.idle":"2024-06-17T14:14:59.709463Z","shell.execute_reply":"2024-06-17T14:14:59.708511Z","shell.execute_reply.started":"2024-06-17T14:14:59.705213Z"},"papermill":{"duration":1.224774,"end_time":"2024-02-29T09:36:31.21757","exception":false,"start_time":"2024-02-29T09:36:29.992796","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","PRIVATE = True\n","\n","# df = pd.read_csv('./input/ai-mathematical-olympiad-prize/test.csv')\n","# df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T16:57:03.930642Z","iopub.status.busy":"2024-05-05T16:57:03.930334Z","iopub.status.idle":"2024-05-05T16:57:03.949882Z","shell.execute_reply":"2024-05-05T16:57:03.949096Z","shell.execute_reply.started":"2024-05-05T16:57:03.930618Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>problem</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>229ee8</td>\n","      <td>Let $k, l &gt; 0$ be parameters. The parabola $y ...</td>\n","      <td>52</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>246d26</td>\n","      <td>Each of the three-digits numbers $111$ to $999...</td>\n","      <td>250</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2fc4ad</td>\n","      <td>Let the `sparkle' operation on positive intege...</td>\n","      <td>702</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>430b63</td>\n","      <td>What is the minimum value of $5x^2+5y^2-8xy$ w...</td>\n","      <td>800</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5277ed</td>\n","      <td>There exists a unique increasing geometric seq...</td>\n","      <td>211</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id                                            problem  answer\n","0  229ee8  Let $k, l > 0$ be parameters. The parabola $y ...      52\n","1  246d26  Each of the three-digits numbers $111$ to $999...     250\n","2  2fc4ad  Let the `sparkle' operation on positive intege...     702\n","3  430b63  What is the minimum value of $5x^2+5y^2-8xy$ w...     800\n","4  5277ed  There exists a unique increasing geometric seq...     211"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# PRIVATE = True\n","df = pd.read_csv('./input/ai-mathematical-olympiad-prize/train.csv')\n","df.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:19:17.265368Z","iopub.status.busy":"2024-06-17T14:19:17.264623Z","iopub.status.idle":"2024-06-17T14:19:17.271565Z","shell.execute_reply":"2024-06-17T14:19:17.270717Z","shell.execute_reply.started":"2024-06-17T14:19:17.265336Z"},"trusted":true},"outputs":[],"source":["def naive_parse(answer):\n","    '''\n","    get the first consecutive numbers\n","    '''\n","    out = []\n","    start = False\n","    end = False\n","    for l in reversed(list(answer)):\n","        if l in '0123456789' and not end:\n","            start = True\n","            out.append(l)\n","        else:\n","            if start:\n","                end = True\n","        \n","    out = reversed(out)\n","    return ''.join(out)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T16:57:03.961113Z","iopub.status.busy":"2024-05-05T16:57:03.960834Z","iopub.status.idle":"2024-05-05T16:57:03.978771Z","shell.execute_reply":"2024-05-05T16:57:03.977925Z","shell.execute_reply.started":"2024-05-05T16:57:03.961091Z"},"trusted":true},"outputs":[],"source":["import re\n","import sys\n","import subprocess\n","\n","def return_last_print(output, n):\n","    lines = output.strip().split('\\n')\n","    if lines:\n","        return lines[n]\n","    else:\n","        return \"\"\n","\n","def process_code(code, return_shell_output=False):\n","    \n","    def repl(match):\n","        if \"real\" not in match.group():\n","            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n","        else:\n","            return \"{}{}\".format(match.group()[:-1], ')')\n","    code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n","\n","    if return_shell_output:\n","        code = code.replace('\\n', '\\n    ')\n","            # Add a try...except block\n","        code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n","    \n","    if not return_shell_output:\n","        print(code)\n","    with open('code.py', 'w') as fout:\n","        fout.write(code)\n","    \n","    batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n","    try:\n","        shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n","        return_value = return_last_print(shell_output, -1)\n","        print(shell_output)\n","        if return_shell_output:\n","            if return_value=='FAIL':\n","                CODE_STATUS = False\n","                return_value = return_last_print(shell_output, -2)\n","                if \"not defined\" in return_value:\n","                    return_value+='\\nTry checking the formatting and imports'\n","            else:\n","                CODE_STATUS = True\n","            return return_value, CODE_STATUS  \n","        code_output = round(float(eval(return_value))) % 1000\n","    except Exception as e:\n","        print(e,'shell_output')\n","        code_output = -1\n","    \n","    if return_shell_output:\n","        if code_output==-1:\n","            CODE_STATUS = False\n","        else:\n","            CODE_STATUS = True\n","        return code_output, CODE_STATUS  \n","    \n","    \n","    return code_output\n","\n","\n","def process_text_output(output):\n","    result = output    \n","    try:\n","        result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n","\n","        print('BOXED', result_output)\n","        # if not len(result_output):\n","        #     result_output = naive_parse(result) # this is toxic\n","        # else:\n","        result_output = result_output[-1]\n","\n","        print('BOXED FINAL', result_output)\n","        if not len(result_output):\n","            result_output = -1\n","        \n","        else:\n","            result_output = round(float(eval(result_output))) % 1000\n","    \n","    except Exception as e:\n","        print(e)\n","        print('ERROR PARSING TEXT')\n","        result_output = -1\n","    \n","    return result_output\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T16:57:03.979978Z","iopub.status.busy":"2024-05-05T16:57:03.9797Z","iopub.status.idle":"2024-05-05T16:57:04.220476Z","shell.execute_reply":"2024-05-05T16:57:04.219456Z","shell.execute_reply.started":"2024-05-05T16:57:03.97995Z"},"trusted":true},"outputs":[{"data":{"text/plain":["20"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T16:57:04.223341Z","iopub.status.busy":"2024-05-05T16:57:04.221614Z","iopub.status.idle":"2024-05-05T16:57:04.229038Z","shell.execute_reply":"2024-05-05T16:57:04.22822Z","shell.execute_reply.started":"2024-05-05T16:57:04.223316Z"},"trusted":true},"outputs":[],"source":["import re\n","import math\n","import random\n","\n","from collections import defaultdict\n","\n","# n_repetitions = 12 if PRIVATE else 4 # Original notebook had 22 but times out :(\n","n_repetitions = 8 if PRIVATE else 4 # Original notebook had 22 but times out :(\n","\n","TOTAL_TOKENS = 2048 # if PRIVATE else 512\n","# TOTAL_TOKENS = 512 # if PRIVATE else 512\n","\n","\n","if PRIVATE:\n","    TIME_LIMIT = 31500\n","else:\n","    TIME_LIMIT = 1"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T06:53:23.248311Z","iopub.status.busy":"2024-05-04T06:53:23.248014Z","iopub.status.idle":"2024-05-04T06:56:25.02544Z","shell.execute_reply":"2024-05-04T06:56:25.024594Z","shell.execute_reply.started":"2024-05-04T06:53:23.248286Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d66e47c9116d4daab688db38f0dac93d","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#list number of cuda\n","n_gpus = torch.cuda.device_count()\n","device_i = 0\n","if n_gpus > 1:\n","    device_i = 1\n","\n","\n","\n","if PRIVATE:\n","\n","    MODEL_PATH = \"./input/deepseek-math\"#\"/kaggle/input/gemma/transformers/7b-it/1\"\n","    DEEP = True\n","\n","    config = AutoConfig.from_pretrained(MODEL_PATH)\n","    config.gradient_checkpointing = True\n","\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","\n","    device_map = [('model.embed_tokens', 0),\n","                 ('model.layers.0', 0),\n","                 ('model.layers.1', 0),\n","                 ('model.layers.2', 0),\n","                 ('model.layers.3', 0),\n","                 ('model.layers.4', 0),\n","                 ('model.layers.5', 0),\n","                 ('model.layers.6', 0),\n","                 ('model.layers.7', 0),\n","                 ('model.layers.8', 0),\n","                 ('model.layers.9', 0),\n","                 ('model.layers.10', 0),\n","                 ('model.layers.11', 0),\n","                 ('model.layers.12', 0),\n","                 ('model.layers.13', 0),\n","                 ('model.layers.14', 0),\n","                 ('model.layers.15', 0),\n","                 ('model.layers.16', 0),\n","                 ('model.layers.17', 0),\n","                 ('model.layers.18', 0),\n","                 ('model.layers.19', 0),\n","                 ('model.layers.20', 0),\n","                 ('model.layers.21', 0),\n","                 ('model.layers.22', device_i),\n","                 ('model.layers.23', device_i),\n","                 ('model.layers.24', device_i),\n","                 ('model.layers.25', device_i),\n","                 ('model.layers.26', device_i),\n","                 ('model.layers.27', device_i),\n","                 ('model.layers.28', device_i),\n","                 ('model.layers.29', device_i),\n","                 ('model.norm', device_i),\n","                 ('lm_head', device_i)]\n","\n","    device_map = {ii:jj for (ii,jj) in device_map}\n","\n","    if QUANT:\n","        from transformers import BitsAndBytesConfig\n","        quantization_config = BitsAndBytesConfig(\n","            load_in_4bit = True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.bfloat16,\n","            bnb_4bit_use_double_quant=True,\n","        )\n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_PATH,\n","            device_map=\"sequential\",\n","            torch_dtype=\"auto\",\n","            trust_remote_code=True, \n","            quantization_config=quantization_config,\n","            config=config\n","        )\n","    else:  \n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_PATH,\n","            device_map=device_map,\n","            torch_dtype=\"auto\",\n","            trust_remote_code=True,\n","            #quantization_config=quantization_config,\n","            config=config\n","        )\n","    \n","    pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype='auto',\n","    device_map=device_map,\n",")\n","    from transformers import StoppingCriteriaList\n","\n","    class StoppingCriteriaSub(StoppingCriteria):\n","        def __init__(self, stops = [], encounters=1):\n","            super().__init__()\n","            self.stops = [stop.to(\"cuda\") for stop in stops]\n","\n","        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n","            for stop in self.stops:\n","                last_token = input_ids[0][-len(stop):]\n","                if torch.all(torch.eq(stop,last_token)):\n","                    return True\n","            return False\n","\n","\n","    stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"] #,  \n","    stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n","    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n","    \n","    model.dtype, model.hf_device_map"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T06:56:25.029669Z","iopub.status.busy":"2024-05-04T06:56:25.029303Z","iopub.status.idle":"2024-05-04T06:56:25.035377Z","shell.execute_reply":"2024-05-04T06:56:25.034298Z","shell.execute_reply.started":"2024-05-04T06:56:25.029643Z"},"trusted":true},"outputs":[],"source":["\n","code = \\\n","\"\"\"Below is a math problem you are to solve (positive numerical answer):\n","\\\"{}\\\"\n","To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\n","Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n","\n","Approach:\"\"\"\n","\n","\n","cot = \\\n","\"\"\"Below is a math problem you are to solve (positive numerical answer!):\n","\\\"{}\\\"\n","Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n","\n","# cot = \\\n","# \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n","# \\\"{}\\\"\n","# Analyze this problem and think step by step to come to a solution. You may use python to assist with solving it. Output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n","\n","# cot =\\\n","# '''\n","# **Problem Statement**:\n","# \\\"{}\\\"\n","\n","# **Request**:\n","# Imagine you are an experienced math tutor. Please provide a comprehensive solution to the IMO math problem stated above. Include detailed explanations of each step and employ necessary mathematical concepts and techniques. Highlight key insights or techniques that are crucial for understanding and solving this type of problem.\n","# '''\n","\n","\n","#     code = \\\n","# \"\"\"\n","# 假设你是IMO数学竞赛指导老师，以下是一个数学问题：\n","# \\\"{}\\\"\n","# \\n\n","# 1. 请先把这个问题翻译成中文。\n","# 2. 接着，确定一个基于sympy，numpy，或者scipy的解题方法，列出每一步的操作。\n","# 3. 请编对应的python脚本，并打印结果。\n","# 4. 你的最终答案应该是正整数，而不是代数表达式！\n","# 5. 解决问题后，在\\\\boxed{}中输出最终的数值答案。\n","\n","# 解：\n","# \"\"\"\n","#     cot = \\\n","# \"\"\"\n","# 假设你是IMO数学竞赛指导老师，以下是一个数学问题：\n","# \\\"{}\\\"\n","# \\n\n","# 1. 请先把这个问题翻译成中文。\n","# 2. 分析这个问题，明确问题的要求与条件，并确定解题思路。\n","# 3. 尝试一步步解决这个问题，在必要的地方列式计算。 \n","# 4. 必要时，可以通过python脚本来辅助计算。\n","# 5. 解决问题后，在\\\\boxed{}中输出最终的数值答案。\n","\n","# 解：\n","# \"\"\"\n","\n","\n","prompt_options = [code,cot]\n","# prompt_options = [code]\n"]},{"cell_type":"code","execution_count":15,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-05-04T06:56:25.037129Z","iopub.status.busy":"2024-05-04T06:56:25.036853Z","iopub.status.idle":"2024-05-04T07:00:32.65925Z","shell.execute_reply":"2024-05-04T07:00:32.658351Z","shell.execute_reply.started":"2024-05-04T06:56:25.037105Z"},"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"109d6bac354144a9a763a5441793e3b1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"]},{"data":{"application/javascript":"IPython.notebook.save_checkpoint();","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Notebook copied to all_captured_20240620-184323_T=735_8_2048_False_0.9|1.1|0.9|1.1/notebook.ipynb\n"]}],"source":["import re\n","from collections import defaultdict\n","from collections import Counter\n","\n","from numpy.random import choice\n","import numpy as np\n","from IPython.utils import io\n","from tqdm.auto import tqdm\n","\n","# tool_instruction = '\\n\\nPlease integrate natural language reasoning with programs to solve the above problem, and put your final numerical answer within \\\\boxed{}.\\nNote that the intermediary calculations may be real numbers, but the final numercal answer would always be an integer.'\n","\n","\n","# temperature = 2.0\n","temperature = 1\n","# top_p = 1.0\n","top_p = 1.1\n","\n","\n","temperature_coding = 1 # code need to be strict, but still should have some creativity to get through difficult problems\n","top_p_coding = 1.1\n","\n","   \n","total_results = {}\n","total_answers = {}\n","best_stats = {}\n","total_outputs = {}\n","question_type_counts = {}\n","starting_counts = (2,3)\n","\n","all_captured = []\n","\n","import datetime\n","timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","\n","all_answers = {}\n","\n","for i, (test, sample_submission) in tqdm(enumerate(iter_test), total=10):\n","    # iterate through every problems in the environment\n","    # use a loop get the test row and the corresponding sample_submissions row\n","\n","    with io.capture_output() as captured: # i capture the printouts\n","\n","        print(f\"Solving problem {i} ...\")\n","        TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n","\n","        if TIME_SPENT>TIME_LIMIT: # submit the prediction if no time is left\n","            sample_submission['answer'] = 0\n","            env.predict(sample_submission)\n","            break\n","            \n","        for trial_j in tqdm(range(n_repetitions)):   \n","\n","            problem = test['problem'].values[0]\n","            print(f\"\\n\\n\\nQUESTION {i} - {trial_j} - TIME_SPENT : {TIME_SPENT:.0f} secs\")\n","            \n","            best, best_count = best_stats.get(i,(-1,-1))\n","            if best_count>np.sqrt(trial_j)+1:\n","                print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n","                continue\n","                \n","            outputs = total_outputs.get(i,[])\n","            text_answers, code_answers = question_type_counts.get(i,starting_counts)\n","            results = total_results.get(i,[])\n","            answers = total_answers.get(i,[])\n","            \n","            for _ in range(5): # clean the cache\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                time.sleep(0.2)\n","\n","            try:\n","                ALREADY_GEN = 0\n","                code_error = None\n","                code_error_count = 0\n","                code_output = -1\n","\n","                counts = np.array([text_answers,code_answers])\n","\n","                # draw = choice(prompt_options, 1,\n","                #             p=counts/counts.sum())\n","                draw = [prompt_options[trial_j%len(prompt_options)]] # alternating \n","\n","                initail_message = draw[0].format(problem,\"{}\")            \n","                prompt = f\"User: {initail_message}\"\n","\n","                current_printed = len(prompt)\n","                print(f\"{trial_j}_{prompt}\\n\")\n","\n","                model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","                input_len = len(model_inputs['input_ids'][0])\n","\n","                generation_output = model.generate(**model_inputs, \n","                                                max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n","                                                return_dict_in_generate=USE_PAST_KEY,\n","                                                do_sample = True,\n","                                                temperature = temperature,\n","                                                top_p = top_p,\n","                                                num_return_sequences=1,\n","                                                stopping_criteria = stopping_criteria)\n","\n","                if USE_PAST_KEY:\n","                    output_ids = generation_output.sequences[0]\n","                else:\n","                    output_ids = generation_output[0]\n","                decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","                print(f\"{decoded_output[current_printed:]}\\n\")\n","                current_printed += len(decoded_output[current_printed:])\n","                cummulative_code = \"\"\n","                \n","                \n","                stop_word_cond = False\n","                for stop_word in stop_words:\n","                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","                    \n","                \n","                while (stop_word_cond) and (ALREADY_GEN<(TOTAL_TOKENS)):\n","\n","                    if (decoded_output[-len(\"```python\"):]==\"```python\"):\n","                        temperature_inner=temperature_coding\n","                        top_p_inner = top_p_coding\n","                        prompt = decoded_output\n","                    else:\n","                        temperature_inner=temperature\n","                        top_p_inner = top_p\n","                        try:\n","                            if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n","                                code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n","                            else:\n","                                code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n","                            \n","\n","                            cummulative_code+=code_text\n","                            code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n","                            print('CODE RESULTS', code_output)\n","                            \n","                            # TODO: try to add the following prompt:\n","                            # Senario: CODE RESULTS 888.888888888889\n","                            # The result is not a positive integer, so we must have made a mistake somewhere. Let's go back and check our work.\n","\n","\n","                            if code_error==code_output:\n","                                code_error_count+=1\n","                            else:\n","                                code_error=code_output\n","                                code_error_count = 0\n","\n","                            if not CODE_STATUS:\n","                                cummulative_code = cummulative_code[:-len(code_text)]\n","\n","                                if code_error_count>=1:\n","                                    print(\"REPEATED ERRORS\")\n","                                    break\n","\n","                        except Exception as e:\n","                            print(e)\n","                            print('ERROR PARSING CODE')\n","                            code_output = -1\n","\n","                       \n","\n","                        if code_output!=-1:\n","                            self_correcting_prompt = ''\n","                            # try:\n","                            #     if int(code_output) != code_output:\n","                            #         self_correcting_prompt=\"\\nThe result is not a positive integer, so we must have made a mistake somewhere. Let's go back and check our work.\"\n","                            # except:\n","                            #     pass\n","\n","                            if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n","                                prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'+self_correcting_prompt\n","                            else:\n","                                prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n' +self_correcting_prompt\n","                        else:\n","                            break # dont fix it lol, it's useless\n","                            prompt = decoded_output + self_correcting_prompt\n","                            cummulative_code=\"\"\n","\n","\n","\n","                    model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","                    ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n","\n","                    if USE_PAST_KEY:\n","                        old_values = generation_output.past_key_values\n","                    else:\n","                        old_values = None\n","\n","                    generation_output = model.generate(**model_inputs, \n","                                                    max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n","                                                    return_dict_in_generate=USE_PAST_KEY,\n","                                                    past_key_values=old_values,\n","                                                    do_sample = True,\n","                                                    temperature = temperature_inner,\n","                                                    top_p = top_p_inner,\n","                                                    num_return_sequences=1, \n","                                                    stopping_criteria = stopping_criteria)\n","\n","                    if USE_PAST_KEY:\n","                        output_ids = generation_output.sequences[0]\n","                    else:\n","                        output_ids = generation_output[0]\n","                    decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","                    print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n","                    current_printed+=len(decoded_output[current_printed:])\n","                    \n","                    stop_word_cond = False\n","                    for stop_word in stop_words:\n","                        stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","\n","                if USE_PAST_KEY:\n","                    output_ids = generation_output.sequences[0]\n","                else:\n","                    output_ids = generation_output[0]\n","\n","                raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n","                #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n","                result_output = process_text_output(raw_output)\n","                \n","                try:\n","                    code_output = round(float(eval(code_output))) % 1000\n","                except Exception as e:\n","                    print(e,'final_eval')\n","                    code_output = -1\n","\n","            except Exception as e:\n","                print(e,\"5\")\n","                result_output, code_output = -1, -1\n","\n","            if code_output!=-1:\n","                outputs.append(code_output)\n","                code_answers+=1\n","\n","            if result_output!=-1:\n","                outputs.append(result_output)\n","                text_answers+=1\n","\n","            if len(outputs) > 0:\n","                occurances = Counter(outputs).most_common()\n","                print(occurances)\n","                if occurances[0][1] > best_count:\n","                    print(\"GOOD ANSWER UPDATED!\")\n","                    best = occurances[0][0]\n","                    best_count = occurances[0][1]\n","                if occurances[0][1] > 5:\n","                    print(\"ANSWER FOUND!\")\n","                    break\n","\n","            results.append(result_output)\n","            answers.append(code_output)\n","            \n","            best_stats[i] = (best, best_count) \n","            question_type_counts[i] = (text_answers, code_answers)\n","            total_outputs[i] = outputs\n","            \n","            total_results[i] = results\n","            total_answers[i] = answers\n","\n","            print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n","            if DEBUG:\n","                break\n","                \n","        print(f\"Predicted best answer: {best_stats}\")\n","        sample_submission['answer'] = best_stats[i][0]\n","        env.predict(sample_submission)\n","\n","    cur_process = captured.stdout\n","    cur_process += '\\n==sep==\\n'\n","    with open(f'outputs_{i}.txt', 'w') as fh:\n","        fh.write(cur_process)\n","    all_captured.append(cur_process)\n","\n","\n","temperatures = '|'.join(map(str, [temperature, top_p, temperature_coding, top_p_coding]))\n","folder_name = f'all_captured_{timestamp}_T={int(TIME_SPENT)}_{n_repetitions}_{TOTAL_TOKENS}_{QUANT}_{temperatures}'\n","\n","import os\n","os.makedirs(folder_name, exist_ok=True)\n","for i, captured in enumerate(all_captured):\n","    with open(f'{folder_name}/output_{i}.txt', 'w') as fh:\n","        fh.write(captured)\n","    # change to md\n","    os.rename(f'{folder_name}/output_{i}.txt', f'{folder_name}/output_{i}.md')\n","\n","all_captured_txt = '\\n'.join(all_captured)\n","with open(f'{folder_name}/all_outputs.txt', 'w') as fh:\n","    fh.write(all_captured_txt)\n","\n","\n","import shutil\n","import os\n","from IPython.display import display, Javascript\n","\n","def save_and_copy_notebook(source_path, destination_path):\n","    # Save the notebook first\n","    display(Javascript('IPython.notebook.save_checkpoint();'))\n","    \n","    # Ensure the notebook is saved before copying\n","    import time\n","    time.sleep(3)  # Wait for a few seconds before copying\n","\n","    # Copy the notebook\n","    shutil.copy2(source_path, destination_path)\n","    print(f\"Notebook copied to {destination_path}\")\n","\n","# Usage example: specify the current notebook path and the target location\n","current_notebook_path = f'./updated-code-interpretation-n-repetitions-17.ipynb'\n","new_location_path = f'{folder_name}/notebook.ipynb'\n","save_and_copy_notebook(current_notebook_path, new_location_path)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["\n","# import os\n","# os.makedirs(folder_name, exist_ok=True)\n","# for i, captured in enumerate(all_captured):\n","#     with open(f'{folder_name}/output_{i}.txt', 'w') as fh:\n","#         fh.write(captured)\n","#     # change to md\n","#     os.rename(f'{folder_name}/output_{i}.txt', f'{folder_name}/output_{i}.md')\n","\n","# all_captured = '\\n'.join(all_captured)\n","# with open(f'{folder_name}/all_outputs.txt', 'w') as fh:\n","#     fh.write(all_captured)\n","\n","\n","# import shutil\n","# import os\n","\n","# from IPython.display import display, Javascript\n","\n","# def save_and_copy_notebook(source_path, destination_path):\n","#     # Save the notebook first\n","#     display(Javascript('IPython.notebook.save_checkpoint();'))\n","    \n","#     # Ensure the notebook is saved before copying\n","#     import time\n","#     time.sleep(3)  # Wait for a few seconds before copying\n","\n","#     # Copy the notebook\n","#     shutil.copy2(source_path, destination_path)\n","#     print(f\"Notebook copied to {destination_path}\")\n","\n","# # Usage example: specify the current notebook path and the target location\n","# current_notebook_path = f'./updated-code-interpretation-n-repetitions-17.ipynb'\n","# new_location_path = f'{folder_name}/notebook.ipynb'\n","# save_and_copy_notebook(current_notebook_path, new_location_path)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.666603Z","iopub.status.busy":"2024-05-04T07:00:32.66628Z","iopub.status.idle":"2024-05-04T07:00:32.680081Z","shell.execute_reply":"2024-05-04T07:00:32.6793Z","shell.execute_reply.started":"2024-05-04T07:00:32.666572Z"},"trusted":true},"outputs":[],"source":["# if PRIVATE:\n","#     df['answer'] = [best_stats[ii][0] for ii in range(len(df))]\n","# else:\n","#     df['answer'] = 2"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.681659Z","iopub.status.busy":"2024-05-04T07:00:32.681425Z","iopub.status.idle":"2024-05-04T07:00:32.690243Z","shell.execute_reply":"2024-05-04T07:00:32.689341Z","shell.execute_reply.started":"2024-05-04T07:00:32.681638Z"},"papermill":{"duration":0.021128,"end_time":"2024-02-29T09:37:05.574782","exception":false,"start_time":"2024-02-29T09:37:05.553654","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# df[['id','answer']].to_csv(\"foo.csv\", header=True, index=False)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.692931Z","iopub.status.busy":"2024-05-04T07:00:32.6913Z","iopub.status.idle":"2024-05-04T07:00:32.701164Z","shell.execute_reply":"2024-05-04T07:00:32.700328Z","shell.execute_reply.started":"2024-05-04T07:00:32.692898Z"},"trusted":true},"outputs":[],"source":["# if not PRIVATE:\n","#     df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n","#     if PRIVATE:\n","#         df['model_answer'] = [best_stats[ii][0] for ii in range(len(df))]\n","#         df['match'] = df.answer == df.model_answer\n","#         print(f'{df.match.sum()} matches in {len(df)} examples')"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.702653Z","iopub.status.busy":"2024-05-04T07:00:32.702324Z","iopub.status.idle":"2024-05-04T07:00:32.819166Z","shell.execute_reply":"2024-05-04T07:00:32.818269Z","shell.execute_reply.started":"2024-05-04T07:00:32.702621Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n","\n"]}],"source":["with open('code.py', 'w') as fout:\n","    fout.write(\"print('done')\")\n","\n","batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n","try:\n","    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n","    print(shell_output)\n","except:\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4281572,"sourceId":7369493,"sourceType":"datasetVersion"},{"datasetId":4720595,"sourceId":8012825,"sourceType":"datasetVersion"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4748944,"sourceId":8052555,"sourceType":"datasetVersion"},{"modelInstanceId":8332,"sourceId":11261,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":11264,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
