{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Forked from https://www.kaggle.com/code/abdurrafae/improved-code-interpretation"]},{"cell_type":"markdown","metadata":{},"source":["**Lewis:** the only changes in this notebook are those needed to run the original one with the new Kaggle evaluation API"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["python版本： 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0]\n","torch版本：2.1.2\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_21204/3843633427.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n","  import pkg_resources\n"]},{"name":"stdout","output_type":"stream","text":["CUDA版本： 12.1\n"]}],"source":["%reload_ext autoreload\n","%autoreload 2\n","import sys\n","print('python版本：',sys.version)\n","\n","import pkg_resources\n","\n","def get_package_version(package_name):\n","    try:\n","        version = pkg_resources.get_distribution(package_name).version\n","        return version\n","    except pkg_resources.DistributionNotFound:\n","        return \"Package not found\"\n","package_name = \"torch\"\n","version = get_package_version(package_name)\n","print(f\"{package_name}版本：{version}\")\n","\n","import torch\n","\n","cuda_version = torch.version.cuda\n","print(\"CUDA版本：\", cuda_version)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["('4.39.3', '0.0.23.post1')"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import transformers\n","import xformers\n","transformers.__version__, xformers.__version__"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:30.874877Z","iopub.status.busy":"2024-06-17T14:14:30.874463Z","iopub.status.idle":"2024-06-17T14:14:30.879196Z","shell.execute_reply":"2024-06-17T14:14:30.878315Z","shell.execute_reply.started":"2024-06-17T14:14:30.874851Z"},"trusted":true},"outputs":[],"source":["## Forked From  https://kaggle.com/code/xiaoz259/pure-rng/notebook\n","\n","\n","# credits:\n","# https://www.kaggle.com/code/olyatsimboy/aimo-openmath-mistral-baseline\n","# https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama\n","# https://www.kaggle.com/code/thedrcat/aimo-mixtral-baseline"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:30.880933Z","iopub.status.busy":"2024-06-17T14:14:30.880566Z","iopub.status.idle":"2024-06-17T14:14:30.897104Z","shell.execute_reply":"2024-06-17T14:14:30.896170Z","shell.execute_reply.started":"2024-06-17T14:14:30.880902Z"},"trusted":true},"outputs":[],"source":["import time\n","\n","NOTEBOOK_START_TIME = time.time()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:30.899592Z","iopub.status.busy":"2024-06-17T14:14:30.899345Z","iopub.status.idle":"2024-06-17T14:14:31.357778Z","shell.execute_reply":"2024-06-17T14:14:31.356805Z","shell.execute_reply.started":"2024-06-17T14:14:30.899570Z"},"trusted":true},"outputs":[],"source":["import aimo\n","try:\n","    env = aimo.make_env()\n","except:\n","    pass\n","iter_test = env.iter_test()\n","\n","from utils import *"]},{"cell_type":"markdown","metadata":{},"source":["TO-DO\n","\n","Change temperature as the question goes longer\n","Change temperature based on question lenght"]},{"cell_type":"markdown","metadata":{},"source":["# Zero-shot MMOS-DeepSeekMath-7B with self-consistency and generated code reasoning evaluation\n","\n","Self-consistency is a modification of the standard greedy decoding in reasoning pipelines via sampling several diverse answers followed by aggregation, e.g., most common answer ([SC-CoT paper](https://arxiv.org/pdf/2203.11171.pdf)).\n","\n","In this kernel, we will consider MMOS-DeepSeekMath-7B RL-tuned backbone; in my experiments, this model produces more consistent code reasoning and the code block execution will allow us to decrease arithmetic hallucinations."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:50.725896Z","iopub.status.busy":"2024-06-17T14:14:50.724898Z","iopub.status.idle":"2024-06-17T14:14:50.730451Z","shell.execute_reply":"2024-06-17T14:14:50.729576Z","shell.execute_reply.started":"2024-06-17T14:14:50.725858Z"},"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["DEBUG = False\n","\n","QUANT = False\n","\n","if QUANT:\n","    from transformers import BitsAndBytesConfig\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit = True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_use_double_quant=True,\n","    )\n","\n","USE_PAST_KEY = True"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:31.367285Z","iopub.status.busy":"2024-06-17T14:14:31.366914Z","iopub.status.idle":"2024-06-17T14:14:50.722689Z","shell.execute_reply":"2024-06-17T14:14:50.721800Z","shell.execute_reply.started":"2024-06-17T14:14:31.367253Z"},"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Transformers Version: 4.39.3\n","CPU times: user 30.7 ms, sys: 30.9 ms, total: 61.6 ms\n","Wall time: 202 ms\n"]}],"source":["%%time\n","if QUANT:\n","    !pip install -U ./input/accelerate-wheelwhl/accelerate-0.29.1-py3-none-any.whl -qq\n","    !pip install -U ./input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq\n","\n","\n","import torch\n","import gc\n","'''\n","accelerate execution a bit\n","'''\n","torch.backends.cuda.enable_mem_efficient_sdp(False)\n","\n","from transformers import (\n","    AutoModelForCausalLM, \n","    AutoTokenizer, \n","    AutoConfig,\n","    StoppingCriteria,\n","    set_seed\n",")\n","\n","import transformers\n","print(f\"Transformers Version: {transformers.__version__}\")\n","set_seed(42)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:59.705246Z","iopub.status.busy":"2024-06-17T14:14:59.704540Z","iopub.status.idle":"2024-06-17T14:14:59.709463Z","shell.execute_reply":"2024-06-17T14:14:59.708511Z","shell.execute_reply.started":"2024-06-17T14:14:59.705213Z"},"papermill":{"duration":1.224774,"end_time":"2024-02-29T09:36:31.21757","exception":false,"start_time":"2024-02-29T09:36:29.992796","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","PRIVATE = True\n","\n","# df = pd.read_csv('./input/ai-mathematical-olympiad-prize/test.csv')\n","# df.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T16:57:03.961113Z","iopub.status.busy":"2024-05-05T16:57:03.960834Z","iopub.status.idle":"2024-05-05T16:57:03.978771Z","shell.execute_reply":"2024-05-05T16:57:03.977925Z","shell.execute_reply.started":"2024-05-05T16:57:03.961091Z"},"trusted":true},"outputs":[],"source":["# def naive_parse(answer):\n","#     '''\n","#     get the first consecutive numbers\n","#     '''\n","#     out = []\n","#     start = False\n","#     end = False\n","#     answer = answer.split('answer')[-1]\n","#     for l in reversed(list(answer)):\n","#         if l in '0123456789' and not end:\n","#             start = True\n","#             out.append(l)\n","#         else:\n","#             if start:\n","#                 end = True\n","        \n","#     out = reversed(out)\n","#     return ''.join(out)\n","\n","# import re\n","# import sys\n","# import subprocess\n","\n","# def return_last_print(output, n):\n","#     lines = output.strip().split('\\n')\n","#     if lines:\n","#         return lines[n]\n","#     else:\n","#         return \"\"\n","\n","# def process_code(code, return_shell_output=False):\n","    \n","#     def repl(match):\n","#         if \"real\" not in match.group():\n","#             return \"{}{}\".format(match.group()[:-1], ', real=True)')\n","#         else:\n","#             return \"{}{}\".format(match.group()[:-1], ')')\n","#     code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n","\n","#     if return_shell_output:\n","#         code = code.replace('\\n', '\\n    ')\n","#             # Add a try...except block\n","#         code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n","    \n","#     if not return_shell_output:\n","#         print(code)\n","#     with open('code.py', 'w') as fout:\n","#         fout.write(code)\n","    \n","#     batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n","#     try:\n","#         shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n","#         return_value = return_last_print(shell_output, -1)\n","#         print(shell_output)\n","#         if return_shell_output:\n","#             if return_value=='FAIL':\n","#                 CODE_STATUS = False\n","#                 return_value = return_last_print(shell_output, -2)\n","#                 if \"not defined\" in return_value:\n","#                     return_value+='\\nTry checking the formatting and imports'\n","#             else:\n","#                 CODE_STATUS = True\n","#             return return_value, CODE_STATUS  \n","#         code_output = round(float(eval(return_value))) % 1000\n","#     except Exception as e:\n","#         print(e,'shell_output')\n","#         code_output = -1\n","    \n","#     if return_shell_output:\n","#         if code_output==-1:\n","#             CODE_STATUS = False\n","#         else:\n","#             CODE_STATUS = True\n","#         return code_output, CODE_STATUS  \n","    \n","    \n","#     return code_output\n","\n","\n","# def process_text_output(output):\n","#     result = output    \n","#     try:\n","#         result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n","\n","#         print('BOXED', result_output)\n","#         # if not len(result_output):\n","#         #     result_output = naive_parse(result) # this is toxic\n","#         # else:\n","#         result_output = result_output[-1]\n","\n","#         print('BOXED FINAL', result_output)\n","#         if not len(result_output):\n","#             result_output = -1\n","        \n","#         else:\n","#             result_output = round(float(eval(result_output))) % 1000\n","    \n","#     except Exception as e:\n","#         print(e)\n","#         print('ERROR PARSING TEXT')\n","#         result_output = -1\n","    \n","#     return result_output\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T16:57:03.979978Z","iopub.status.busy":"2024-05-05T16:57:03.9797Z","iopub.status.idle":"2024-05-05T16:57:04.220476Z","shell.execute_reply":"2024-05-05T16:57:04.219456Z","shell.execute_reply.started":"2024-05-05T16:57:03.97995Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T16:57:04.223341Z","iopub.status.busy":"2024-05-05T16:57:04.221614Z","iopub.status.idle":"2024-05-05T16:57:04.229038Z","shell.execute_reply":"2024-05-05T16:57:04.22822Z","shell.execute_reply.started":"2024-05-05T16:57:04.223316Z"},"trusted":true},"outputs":[],"source":["import re\n","import math\n","import random\n","\n","from collections import defaultdict\n","\n","# n_repetitions = 12 if PRIVATE else 4 # Original notebook had 22 but times out :(\n","n_repetitions = 12 if PRIVATE else 4 # Original notebook had 22 but times out :(\n","\n","TOTAL_TOKENS = 1600 # if PRIVATE else 512\n","# TOTAL_TOKENS = 512 # if PRIVATE else 512\n","\n","\n","if PRIVATE:\n","    TIME_LIMIT = 31500\n","else:\n","    TIME_LIMIT = 1"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T06:53:23.248311Z","iopub.status.busy":"2024-05-04T06:53:23.248014Z","iopub.status.idle":"2024-05-04T06:56:25.02544Z","shell.execute_reply":"2024-05-04T06:56:25.024594Z","shell.execute_reply.started":"2024-05-04T06:53:23.248286Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"376d2265b87d429a9496c973ff27b566","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#list number of cuda\n","n_gpus = torch.cuda.device_count()\n","device_i = 0\n","if n_gpus > 1:\n","    device_i = 1\n","\n","\n","\n","if PRIVATE:\n","\n","    MODEL_PATH = \"./input/deepseek-math\"#\"/kaggle/input/gemma/transformers/7b-it/1\"\n","    DEEP = True\n","\n","    config = AutoConfig.from_pretrained(MODEL_PATH)\n","    config.gradient_checkpointing = True\n","\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","\n","    device_map = [('model.embed_tokens', 0),\n","                 ('model.layers.0', 0),\n","                 ('model.layers.1', 0),\n","                 ('model.layers.2', 0),\n","                 ('model.layers.3', 0),\n","                 ('model.layers.4', 0),\n","                 ('model.layers.5', 0),\n","                 ('model.layers.6', 0),\n","                 ('model.layers.7', 0),\n","                 ('model.layers.8', 0),\n","                 ('model.layers.9', 0),\n","                 ('model.layers.10', 0),\n","                 ('model.layers.11', 0),\n","                 ('model.layers.12', 0),\n","                 ('model.layers.13', 0),\n","                 ('model.layers.14', 0),\n","                 ('model.layers.15', 0),\n","                 ('model.layers.16', 0),\n","                 ('model.layers.17', 0),\n","                 ('model.layers.18', 0),\n","                 ('model.layers.19', 0),\n","                 ('model.layers.20', 0),\n","                 ('model.layers.21', 0),\n","                 ('model.layers.22', device_i),\n","                 ('model.layers.23', device_i),\n","                 ('model.layers.24', device_i),\n","                 ('model.layers.25', device_i),\n","                 ('model.layers.26', device_i),\n","                 ('model.layers.27', device_i),\n","                 ('model.layers.28', device_i),\n","                 ('model.layers.29', device_i),\n","                 ('model.norm', device_i),\n","                 ('lm_head', device_i)]\n","\n","    device_map = {ii:jj for (ii,jj) in device_map}\n","\n","    if QUANT:\n","        from transformers import BitsAndBytesConfig\n","        quantization_config = BitsAndBytesConfig(\n","            load_in_4bit = True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.bfloat16,\n","            bnb_4bit_use_double_quant=True,\n","        )\n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_PATH,\n","            device_map=\"sequential\",\n","            torch_dtype=\"auto\",\n","            trust_remote_code=True, \n","            quantization_config=quantization_config,\n","            config=config\n","        )\n","    else:  \n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_PATH,\n","            device_map=device_map,\n","            torch_dtype=\"auto\",\n","            trust_remote_code=True,\n","            #quantization_config=quantization_config,\n","            config=config\n","        )\n","    \n","    pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype='auto',\n","    device_map=device_map,\n",")\n","    from transformers import StoppingCriteriaList\n","\n","    class StoppingCriteriaSub(StoppingCriteria):\n","        def __init__(self, stops = [], encounters=1):\n","            super().__init__()\n","            self.stops = [stop.to(\"cuda\") for stop in stops]\n","\n","        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n","            for stop in self.stops:\n","                last_token = input_ids[0][-len(stop):]\n","                if torch.all(torch.eq(stop,last_token)):\n","                    return True\n","            return False\n","\n","\n","    stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"] #,  \n","    stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n","    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n","    \n","    model.dtype, model.hf_device_map"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T06:56:25.029669Z","iopub.status.busy":"2024-05-04T06:56:25.029303Z","iopub.status.idle":"2024-05-04T06:56:25.035377Z","shell.execute_reply":"2024-05-04T06:56:25.034298Z","shell.execute_reply.started":"2024-05-04T06:56:25.029643Z"},"trusted":true},"outputs":[],"source":["# code = \\\n","# \"\"\"Below is a math problem you are to solve (positive numerical answer):\n","# \\\"{}\\\"\n","# To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\n","# Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n","\n","# Approach:\"\"\"\n","\n","# code = \\\n","# \"\"\"\n","# Below is a math problem you are to solve (positive numerical answer):\n","# \\\"{}\\\"\n","# To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\n","# Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n","\n","# Assistant: \n","# Here is the approach:\"\"\" # seems ok this one\n","\n","\n","\n","\n","# code = \\\n","# \"\"\"Below is a math problem you are to solve (positive numerical answer):\n","# \\\"{}\\\"\n","# To accomplish this, first determine a python-based (e.g. sympy, numpy, scipy ...) approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear and professional so even an idiot can follow your instructions.\n","# Write the entire script covering all the steps and print the result. After solving the problem, output the final numerical answer within this \\\\boxed{}.\\n\\n\"\"\"\n","\n","# code = \\\n","# \"\"\"Below is a math problem you are to solve (positive numerical answer):\n","# \\\"{}\\\"\n","# To accomplish this, first clarify the requirements and constraints of the problem and what steps to take. Be clear and professional so even an idiot can follow your instructions.\n","# Then, derive the solution step by step. You may use python. After solving the problem, output the final numerical answer within this \\\\boxed{}.\\n\\n\"\"\"\n","\n","\n","# cot = \\\n","# \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n","# \\\"{}\\\"\n","# Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n","\n","\n","\n","# cot = \\\n","# \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n","# \\\"{}\\\"\n","# Analyze this problem and think step by step to come to a solution with the help of python. After solving the problem, write the final answer within \\\\boxed{}.\\n\\n\"\"\"\n","\n","# cot = \\\n","# \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n","# \\\"{}\\\"\n","# Analyze this problem and think step by step to come to a solution. You may use python to assist with solving it. Output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n","\n","# cot =\\\n","# '''\n","# **Problem Statement**:\n","# \\\"{}\\\"\n","\n","# **Request**:\n","# Imagine you are an experienced math tutor. Please provide a comprehensive solution to the IMO math problem stated above. Include detailed explanations of each step and employ necessary mathematical concepts and techniques. Highlight key insights or techniques that are crucial for understanding and solving this type of problem.\n","# '''\n","\n","\n","#     code = \\\n","# \"\"\"\n","# 假设你是IMO数学竞赛指导老师，以下是一个数学问题：\n","# \\\"{}\\\"\n","# \\n\n","# 1. 请先把这个问题翻译成中文。\n","# 2. 接着，确定一个基于sympy，numpy，或者scipy的解题方法，列出每一步的操作。\n","# 3. 请编对应的python脚本，并打印结果。\n","# 4. 你的最终答案应该是正整数，而不是代数表达式！\n","# 5. 解决问题后，在\\\\boxed{}中输出最终的数值答案。\n","\n","# 解：\n","# \"\"\"\n","#     cot = \\\n","# \"\"\"\n","# 假设你是IMO数学竞赛指导老师，以下是一个数学问题：\n","# \\\"{}\\\"\n","# \\n\n","# 1. 请先把这个问题翻译成中文。\n","# 2. 分析这个问题，明确问题的要求与条件，并确定解题思路。\n","# 3. 尝试一步步解决这个问题，在必要的地方列式计算。 \n","# 4. 必要时，可以通过python脚本来辅助计算。\n","# 5. 解决问题后，在\\\\boxed{}中输出最终的数值答案。\n","\n","# 解：\n","# \"\"\"\n","\n","code = \\\n","\"\"\"\n","Below is a math problem you are to solve (positive numerical answer):\n","\\\"{}\\\"\n","To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\n","Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n","\n","Assistant: \n","\n","Interesting, let's analyze step by step:\"\"\" # seems ok this one\n","\n","cot = \\\n","\"\"\"\n","\n","Below is a math problem you are to solve (positive numerical answer!):\n","\\\"{}\\\"\n","Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\n","\n","Assistant:\"\"\"\n","prompt_options = [code,cot]\n","# prompt_options = [code]\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf11db37525a4a23a4143f5ae936de92","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import re\n","from collections import defaultdict\n","from collections import Counter\n","\n","from numpy.random import choice\n","import numpy as np\n","from IPython.utils import io\n","from tqdm.auto import tqdm\n","\n","# tool_instruction = '\\n\\nPlease integrate natural language reasoning with programs to solve the above problem, and put your final numerical answer within \\\\boxed{}.\\nNote that the intermediary calculations may be real numbers, but the final numercal answer would always be an integer.'\n","\n","\n","# temperature = 2.0\n","temperature = 1.045922532979117\n","# top_p = 1.0\n","top_p = 0.9194345787064896\n","top_k = 32\n","\n","\n","temperature_coding =  0.8573434329772586# code need to be strict, but still should have some creativity to get through difficult problems\n","top_p_coding = 0.9598891549329412\n","top_k_coding = 32\n","\n","   \n","total_results = {}\n","total_answers = {}\n","best_stats = {}\n","total_outputs = {}\n","question_type_counts = {}\n","starting_counts = (2,2)\n","# starting_counts = (3,2)\n","\n","\n","all_captured = []\n","all_self_reflections = []\n","\n","import datetime\n","timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","\n","all_answers = {}\n","\n","\n","test_df = pd.read_csv('./train.csv')\n","submission_df = pd.read_csv('./sample_submission.csv')\n","\n","final_submission = pd.DataFrame(columns=submission_df.columns)\n","\n","model_score = 0\n","\n","REFLECTION = False\n","TOTAL_REFLECTION_TIME = 0\n","\n","total_prompt_correct_count = [0 for _ in range(len(prompt_options))]\n","for i in tqdm(range(len(test_df))):\n","    test, sample_submission = test_df.loc[i:i, :], submission_df.loc[i:i, :]\n","    # iterate through every problems in the environment\n","    # use a loop get the test row and the corresponding sample_submissions row\n","\n","    total_answers_to_gen = 0\n","    valid_answers_generated = 0\n","    answers_matched = 0\n","    self_reflection_list = []\n","    prompt_correct_count = [0 for _ in range(len(prompt_options))]\n","    with io.capture_output() as captured: # i capture the printouts\n","\n","        print(f\"Solving problem {i} ...\")\n","        TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n","\n","        if TIME_SPENT>TIME_LIMIT: # submit the prediction if no time is left\n","            sample_submission['answer'] = 0\n","            # env.predict(sample_submission)\n","            final_submission = pd.concat([final_submission, sample_submission])\n","            break\n","            \n","        for trial_j in tqdm(range(n_repetitions)):   \n","            problem = test['problem'].values[0]\n","            print(f\"\\n\\n\\n## QUESTION {i} - {trial_j} \\n- TIME_SPENT : {TIME_SPENT:.0f} secs\\n\")\n","            \n","            best, best_count = best_stats.get(i,(-1,-1))\n","            # if best_count>np.sqrt(trial_j)+1:\n","            #     print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n","            #     continue\n","\n","            total_answers_to_gen += 1\n","                \n","            outputs = total_outputs.get(i,[])\n","            text_answers, code_answers = question_type_counts.get(i,starting_counts)\n","            results = total_results.get(i,[])\n","            answers = total_answers.get(i,[])\n","            \n","            for _ in range(5): # clean the cache\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                time.sleep(0.2)\n","\n","            try:\n","                ALREADY_GEN = 0\n","                code_error = None\n","                code_error_count = 0\n","                code_output = -1\n","                counts = np.array([text_answers,code_answers])\n","\n","                draw_index = choice(range(len(prompt_options)), 1, p=counts/counts.sum())[0]\n","                draw = [prompt_options[draw_index]]\n","                # draw = choice(prompt_options, 1,\n","                #             p=counts/counts.sum())\n","                initail_message = draw[0].format(problem,\"{}\")            \n","                prompt = f\"User:\\n{initail_message}\"\n","\n","                current_printed = len(prompt)\n","                print(f\"{trial_j}_{prompt}\\n\")\n","\n","                model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","                input_len = len(model_inputs['input_ids'][0])\n","\n","                generation_output = model.generate(**model_inputs, \n","                                            max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n","                                            return_dict_in_generate=USE_PAST_KEY,\n","                                            do_sample = True,\n","                                            temperature = temperature,\n","                                            top_p = top_p,\n","                                            top_k = top_k,\n","                                            num_return_sequences=1,\n","                                            stopping_criteria = stopping_criteria,\n","                                            pad_token_id=tokenizer.eos_token_id) #\n","\n","                if USE_PAST_KEY:\n","                    output_ids = generation_output.sequences[0]\n","                else:\n","                    output_ids = generation_output[0]\n","                decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","                print(f\"{decoded_output[current_printed:]}\\n\")\n","                current_printed += len(decoded_output[current_printed:])\n","                cummulative_code = \"\"\n","                \n","                \n","                stop_word_cond = False\n","                for stop_word in stop_words:\n","                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","                    \n","                \n","                while (stop_word_cond) and (ALREADY_GEN<(TOTAL_TOKENS)):\n","\n","                    if (decoded_output[-len(\"```python\"):]==\"```python\"):\n","                        temperature_inner=temperature_coding\n","                        top_p_inner = top_p_coding\n","                        top_k_inner = top_k_coding\n","                        prompt = decoded_output\n","                    else:\n","                        temperature_inner=temperature\n","                        top_p_inner = top_p\n","                        top_k_inner = top_k\n","                        try:\n","                            if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n","                                code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n","                            else:\n","                                code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n","                            \n","\n","                            cummulative_code+=code_text\n","                            code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n","                            print('CODE RESULTS', code_output)\n","                            \n","                            # TODO: try to add the following prompt:\n","                            # Senario: CODE RESULTS 888.888888888889\n","                            # The result is not a positive integer, so we must have made a mistake somewhere. Let's go back and check our work.\n","\n","\n","                            if code_error==code_output:\n","                                code_error_count+=1\n","                            else:\n","                                code_error=code_output\n","                                code_error_count = 0\n","\n","                            if not CODE_STATUS:\n","                                cummulative_code = cummulative_code[:-len(code_text)]\n","\n","                                if code_error_count>=2: # more allowance\n","                                    print(\"REPEATED ERRORS\")\n","                                    break\n","\n","                        except Exception as e:\n","                            print(e)\n","                            print('ERROR PARSING CODE')\n","                            code_output = -1\n","\n","                    \n","                        if code_output!=-1:\n","                            self_correcting_prompt = ''\n","                        \n","                            if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n","                                prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'+self_correcting_prompt\n","                            else:\n","                                prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n' +self_correcting_prompt\n","                        else:\n","                            prompt = decoded_output\n","                            cummulative_code=\"\"\n","\n","\n","\n","                    model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","                    ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n","\n","                    if USE_PAST_KEY:\n","                        old_values = generation_output.past_key_values\n","                    else:\n","                        old_values = None\n","\n","                    generation_output = model.generate(**model_inputs, \n","                                                max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n","                                                return_dict_in_generate=USE_PAST_KEY,\n","                                                past_key_values=old_values,\n","                                                do_sample = True,\n","                                                temperature = temperature_inner,\n","                                                top_p = top_p_inner,\n","                                                top_k = top_k_inner,\n","                                                num_return_sequences=1, \n","                                                stopping_criteria = stopping_criteria,\n","                                                pad_token_id=tokenizer.eos_token_id) # for open ended gen\n","\n","\n","                    if USE_PAST_KEY:\n","                        output_ids = generation_output.sequences[0]\n","                    else:\n","                        output_ids = generation_output[0]\n","                    decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","                    print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n","                    current_printed+=len(decoded_output[current_printed:])\n","                    \n","                    stop_word_cond = False\n","                    for stop_word in stop_words:\n","                        stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","\n","                if USE_PAST_KEY:\n","                    output_ids = generation_output.sequences[0]\n","                else:\n","                    output_ids = generation_output[0]\n","\n","                raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n","                result_output = process_text_output(raw_output)\n","                \n","                try:\n","                    code_output = round(float(eval(code_output))) % 1000\n","                except Exception as e:\n","                    print(e,'final_eval')\n","                    code_output = -1\n","\n","            except Exception as e:\n","                print(e,\"5\")\n","                result_output, code_output = -1, -1\n","\n","            valid_answer_obtained = False\n","            if code_output!=-1:\n","                outputs.append(code_output)\n","                code_answers+=1\n","                valid_answer_obtained = True\n","\n","            if result_output!=-1:\n","                outputs.append(result_output)\n","                text_answers+=1\n","                valid_answer_obtained = True\n","\n","            valid_answers_generated += int(valid_answer_obtained)\n","\n","            # self reflection: jason\n","            REFLECTION_TIME_START = time.time()\n","            self_reflection_text = 'None'\n","            if result_output == -1 or result_output != test['answer'].values[0]:\n","                if REFLECTION:\n","                    self_reflection = f\"\\n\\nUser: Thanks for trying. In fact, the correct answer is {test['answer'].values[0]}. Can you briefly summarize what you did wrong and what will you do differently if you are to try again?\\n\\nAssistant:\"\n","                    prompt = decoded_output + self_reflection\n","                    \n","                    model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","                    input_tokens_len = len(model_inputs['input_ids'][0])\n","                    old_values = generation_output.past_key_values\n","                    self_reflection_output = model.generate(**model_inputs, \n","                                                    max_new_tokens=128, \n","                                                    return_dict_in_generate=USE_PAST_KEY,\n","                                                    past_key_values=old_values,\n","                                                    do_sample = True,\n","                                                    temperature = 0.9,\n","                                                    top_p = 1,\n","                                                    num_return_sequences=1, \n","                                                    stopping_criteria = stopping_criteria,\n","                                                    \n","                                                    pad_token_id=tokenizer.eos_token_id) # for open ended gen\n","                    new_content = self_reflection_output.sequences[0][input_tokens_len:]\n","                    self_reflection_text = tokenizer.decode(new_content, skip_special_tokens=True)\n","            else:\n","                prompt_correct_count[draw_index] += 1\n","            self_reflection_text = f'\\n### Question {i} {trial_j} reflection:\\n' + self_reflection_text\n","            self_reflection_list.append(self_reflection_text)\n","            REFLECTION_TIME_END = time.time() - REFLECTION_TIME_START\n","            TOTAL_REFLECTION_TIME += REFLECTION_TIME_END\n","\n","            if len(outputs) > 0:\n","                occurances = Counter(outputs).most_common()\n","                print(occurances)\n","                if occurances[0][1] > best_count:\n","                    print(\"GOOD ANSWER UPDATED!\")\n","                    best = occurances[0][0]\n","                    best_count = occurances[0][1]\n","                    # in case the following break happens while the best changes\n","                    # get the best in advance\n","                    best_stats[i] = (best, best_count) \n","                if occurances[0][1] > 3: # originally 5\n","                    print(\"ANSWER FOUND!\")\n","                    break\n","\n","            results.append(result_output)\n","            answers.append(code_output)\n","            \n","            best_stats[i] = (best, best_count) \n","            question_type_counts[i] = (text_answers, code_answers)\n","            total_outputs[i] = outputs\n","            \n","            total_results[i] = results\n","            total_answers[i] = answers\n","\n","            print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n","            if DEBUG:\n","                break\n","                \n","        print(f\"Predicted best answer: {best_stats}\")\n","        sample_submission['answer'] = best_stats[i][0]\n","        correct_answer = test['answer'].values[0]\n","        current_outputs = total_outputs.get(i, [])\n","        answers_matched = Counter(current_outputs).get(correct_answer, 0)\n","        best_matched = best_stats[i][0] == correct_answer # True when most common is correct\n","\n","        model_score += best_matched * 100 + answers_matched/len(current_outputs) * 10 \\\n","            + valid_answers_generated/total_answers_to_gen * 10\n","\n","        # env.predict(sample_submission)\n","        final_submission = pd.concat([final_submission, sample_submission])\n","\n","    cur_process = captured.stdout\n","    cur_process += '\\nprompt correctness:'+ str(prompt_correct_count)\n","    cur_reflections = ''.join(self_reflection_list)\n","    all_self_reflections.append(cur_reflections)\n","\n","    cur_process += '\\n\\n## Self-Reflections\\n'+cur_reflections\n","    cur_process += '\\n---\\n'\n","\n","    with open(f'outputs_{i}.txt', 'w') as fh:\n","        fh.write(cur_process)\n","    all_captured.append(cur_process)\n","    total_prompt_correct_count = np.array(total_prompt_correct_count) + np.array(prompt_correct_count)\n","\n","TIME_SPENT = time.time() - NOTEBOOK_START_TIME - TOTAL_REFLECTION_TIME\n","\n","model_score = model_score / len(test_df)\n","\n","import json\n","\n","print(json.dumps({'model_score': model_score,\n","                  'prompt_correct': total_prompt_correct_count.tolist(),}))\n","\n","temperatures = '|'.join(map(str, [temperature, top_p, temperature_coding, top_p_coding]))\n","folder_name = f'outputs/all_captured_{timestamp}_T={int(TIME_SPENT)}_{n_repetitions}_{TOTAL_TOKENS}_{temperatures}_{model_score:.2f}'\n","save_current_exp(folder_name, all_captured, final_submission, notebooks=['./main.ipynb', './utils.py'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import re\n","# from collections import defaultdict\n","# from collections import Counter\n","\n","# from numpy.random import choice\n","# import numpy as np\n","# from IPython.utils import io\n","# from tqdm.auto import tqdm\n","\n","# # tool_instruction = '\\n\\nPlease integrate natural language reasoning with programs to solve the above problem, and put your final numerical answer within \\\\boxed{}.\\nNote that the intermediary calculations may be real numbers, but the final numercal answer would always be an integer.'\n","\n","\n","# # temperature = 2.0\n","# temperature = 0.858741856592889\n","# # top_p = 1.0\n","# top_p = 0.8524499927859499\n","# top_k = 48\n","\n","\n","# temperature_coding =  0.6611832012819016# code need to be strict, but still should have some creativity to get through difficult problems\n","# top_p_coding = 0.919453939638894\n","# top_k_coding = 50\n","\n","   \n","# total_results = {}\n","# total_answers = {}\n","# best_stats = {}\n","# total_outputs = {}\n","# question_type_counts = {}\n","# starting_counts = (2,2)\n","# # starting_counts = (3,2)\n","\n","\n","# all_captured = []\n","# all_self_reflections = []\n","\n","# import datetime\n","# timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","\n","# all_answers = {}\n","\n","\n","# test_df = pd.read_csv('./test.csv')\n","# submission_df = pd.read_csv('./sample_submission.csv')\n","\n","# final_submission = pd.DataFrame(columns=submission_df.columns)\n","\n","# model_score = 0\n","\n","# REFLECTION = False\n","# TOTAL_REFLECTION_TIME = 0\n","\n","# total_prompt_correct_count = [0 for _ in range(len(prompt_options))]\n","# for i in tqdm(range(len(test_df))):\n","#     test, sample_submission = test_df.loc[i:i, :], submission_df.loc[i:i, :]\n","#     # iterate through every problems in the environment\n","#     # use a loop get the test row and the corresponding sample_submissions row\n","\n","#     total_answers_to_gen = 0\n","#     valid_answers_generated = 0\n","#     answers_matched = 0\n","#     self_reflection_list = []\n","#     prompt_correct_count = [0 for _ in range(len(prompt_options))]\n","#     with io.capture_output() as captured: # i capture the printouts\n","\n","#         print(f\"\\n# Solving problem {i} ...\")\n","#         TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n","\n","#         if TIME_SPENT>TIME_LIMIT: # submit the prediction if no time is left\n","#             sample_submission['answer'] = 0\n","#             # env.predict(sample_submission)\n","#             final_submission = pd.concat([final_submission, sample_submission])\n","#             break\n","            \n","#         for trial_j in tqdm(range(n_repetitions)):   \n","#             problem = test['problem'].values[0]\n","#             print(f\"\\n\\n\\n## QUESTION {i} - {trial_j} \\n- TIME_SPENT : {TIME_SPENT:.0f} secs\\n\")\n","            \n","#             best, best_count = best_stats.get(i,(-1,-1))\n","#             if best_count>np.sqrt(trial_j) + 1:\n","#                 print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n","#                 continue\n","\n","#             total_answers_to_gen += 1\n","                \n","#             outputs = total_outputs.get(i,[])\n","#             text_answers, code_answers = question_type_counts.get(i,starting_counts)\n","#             results = total_results.get(i,[])\n","#             answers = total_answers.get(i,[])\n","            \n","#             for _ in range(5): # clean the cache\n","#                 torch.cuda.empty_cache()\n","#                 gc.collect()\n","#                 time.sleep(0.2)\n","\n","#             try:\n","#                 ALREADY_GEN = 0\n","#                 code_error = None\n","#                 code_error_count = 0\n","#                 code_output = -1\n","#                 counts = np.array([text_answers,code_answers])\n","\n","#                 draw_index = choice(range(len(prompt_options)), 1, p=counts/counts.sum())[0]\n","#                 draw = [prompt_options[draw_index]]\n","#                 # draw = choice(prompt_options, 1,\n","#                 #             p=counts/counts.sum())\n","#                 initail_message = draw[0].format(problem,\"{}\")            \n","#                 prompt = f\"User:\\n{initail_message}\"\n","\n","#                 current_printed = len(prompt)\n","#                 print(f\"{trial_j}_{prompt}\\n\")\n","\n","#                 model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","#                 input_len = len(model_inputs['input_ids'][0])\n","\n","#                 generation_output = model.generate(**model_inputs, \n","#                                                 max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n","#                                                 return_dict_in_generate=USE_PAST_KEY,\n","#                                                 do_sample = True,\n","#                                                 temperature = temperature,\n","#                                                 top_p = top_p,\n","#                                                 top_k = top_k,\n","#                                                 num_return_sequences=1,\n","#                                                 stopping_criteria = stopping_criteria,\n","#                                                 pad_token_id=tokenizer.eos_token_id) # for open ended gen\n","\n","#                 if USE_PAST_KEY:\n","#                     output_ids = generation_output.sequences[0]\n","#                 else:\n","#                     output_ids = generation_output[0]\n","#                 decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","#                 print(f\"{decoded_output[current_printed:]}\\n\")\n","#                 current_printed += len(decoded_output[current_printed:])\n","#                 cummulative_code = \"\"\n","                \n","                \n","#                 stop_word_cond = False\n","#                 for stop_word in stop_words:\n","#                     stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","                    \n","                \n","#                 while (stop_word_cond) and (ALREADY_GEN<(TOTAL_TOKENS)):\n","\n","#                     if (decoded_output[-len(\"```python\"):]==\"```python\"):\n","#                         temperature_inner=temperature_coding\n","#                         top_p_inner = top_p_coding\n","#                         top_k_inner = top_k_coding\n","#                         prompt = decoded_output\n","#                     else:\n","#                         temperature_inner=temperature\n","#                         top_p_inner = top_p\n","#                         # top_k_inner = top_k # my mistake here...\n","#                         try:\n","#                             if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n","#                                 code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n","#                             else:\n","#                                 code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n","                            \n","\n","#                             cummulative_code+=code_text\n","#                             code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n","#                             print('CODE RESULTS', code_output)\n","                            \n","#                             # TODO: try to add the following prompt:\n","#                             # Senario: CODE RESULTS 888.888888888889\n","#                             # The result is not a positive integer, so we must have made a mistake somewhere. Let's go back and check our work.\n","\n","\n","#                             if code_error==code_output:\n","#                                 code_error_count+=1\n","#                             else:\n","#                                 code_error=code_output\n","#                                 code_error_count = 0\n","\n","#                             if not CODE_STATUS:\n","#                                 cummulative_code = cummulative_code[:-len(code_text)]\n","\n","#                                 if code_error_count>=3: # more allowance\n","#                                     print(\"REPEATED ERRORS\")\n","#                                     break\n","\n","#                         except Exception as e:\n","#                             print(e)\n","#                             print('ERROR PARSING CODE')\n","#                             code_output = -1\n","\n","                    \n","#                         if code_output!=-1:\n","#                             self_correcting_prompt = ''\n","                        \n","#                             if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n","#                                 prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'+self_correcting_prompt\n","#                             else:\n","#                                 prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n' +self_correcting_prompt\n","#                         else:\n","#                             # self_correcting_prompt = \"Let's analyze and fix the error in our code, and then continue to solve the math problem.\"\n","\n","#                             prompt = decoded_output\n","#                             cummulative_code=\"\"\n","#                             # break\n","\n","\n","\n","#                     model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","#                     ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n","\n","#                     if USE_PAST_KEY:\n","#                         old_values = generation_output.past_key_values\n","#                     else:\n","#                         old_values = None\n","\n","#                     generation_output = model.generate(**model_inputs, \n","#                                                     max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n","#                                                     return_dict_in_generate=USE_PAST_KEY,\n","#                                                     past_key_values=old_values,\n","#                                                     do_sample = True,\n","#                                                     temperature = temperature_inner,\n","#                                                     top_p = top_p_inner,\n","#                                                     top_k = top_k_inner,\n","#                                                     num_return_sequences=1, \n","#                                                     stopping_criteria = stopping_criteria,\n","#                                                     pad_token_id=tokenizer.eos_token_id) # for open ended gen\n","\n","#                     if USE_PAST_KEY:\n","#                         output_ids = generation_output.sequences[0]\n","#                     else:\n","#                         output_ids = generation_output[0]\n","#                     decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","#                     print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n","#                     current_printed+=len(decoded_output[current_printed:])\n","                    \n","#                     stop_word_cond = False\n","#                     for stop_word in stop_words:\n","#                         stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","\n","#                 if USE_PAST_KEY:\n","#                     output_ids = generation_output.sequences[0]\n","#                 else:\n","#                     output_ids = generation_output[0]\n","\n","#                 raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n","#                 result_output = process_text_output(raw_output)\n","                \n","#                 try:\n","#                     code_output = round(float(eval(code_output))) % 1000\n","#                 except Exception as e:\n","#                     print(e,'final_eval')\n","#                     code_output = -1\n","\n","#             except Exception as e:\n","#                 print(e,\"5\")\n","#                 result_output, code_output = -1, -1\n","\n","\n","#             valid_answer_obtained = False\n","#             if code_output!=-1:\n","#                 outputs.append(code_output)\n","#                 code_answers+=1\n","#                 valid_answer_obtained = True\n","\n","#             if result_output!=-1:\n","#                 outputs.append(result_output)\n","#                 text_answers+=1\n","#                 valid_answer_obtained = True\n","\n","#             valid_answers_generated += int(valid_answer_obtained)\n","\n","#             # self reflection: jason\n","#             REFLECTION_TIME_START = time.time()\n","#             self_reflection_text = 'None'\n","#             if result_output == -1 or result_output != test['answer'].values[0]:\n","#                 if REFLECTION:\n","#                     self_reflection = f\"\\n\\nUser: Thanks for trying. In fact, the correct answer is {test['answer'].values[0]}. Can you briefly summarize what you did wrong and what will you do differently if you are to try again?\\n\\nAssistant:\"\n","#                     prompt = decoded_output + self_reflection\n","                    \n","#                     model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","#                     input_tokens_len = len(model_inputs['input_ids'][0])\n","#                     old_values = generation_output.past_key_values\n","#                     self_reflection_output = model.generate(**model_inputs, \n","#                                                     max_new_tokens=128, \n","#                                                     return_dict_in_generate=USE_PAST_KEY,\n","#                                                     past_key_values=old_values,\n","#                                                     do_sample = True,\n","#                                                     temperature = 0.9,\n","#                                                     top_p = 1,\n","#                                                     num_return_sequences=1, \n","#                                                     stopping_criteria = stopping_criteria,\n","                                                    \n","#                                                     pad_token_id=tokenizer.eos_token_id) # for open ended gen\n","#                     new_content = self_reflection_output.sequences[0][input_tokens_len:]\n","#                     self_reflection_text = tokenizer.decode(new_content, skip_special_tokens=True)\n","#             else:\n","#                 prompt_correct_count[draw_index] += 1\n","#             self_reflection_text = f'\\n### Question {i} {trial_j} reflection:\\n' + self_reflection_text\n","#             self_reflection_list.append(self_reflection_text)\n","#             REFLECTION_TIME_END = time.time() - REFLECTION_TIME_START\n","#             TOTAL_REFLECTION_TIME += REFLECTION_TIME_END\n","\n","#             if len(outputs) > 0:\n","#                 occurances = Counter(outputs).most_common()\n","#                 print(occurances)\n","#                 if occurances[0][1] > best_count:\n","#                     print(\"GOOD ANSWER UPDATED!\")\n","#                     best = occurances[0][0]\n","#                     best_count = occurances[0][1]\n","#                     # in case the following break happens while the best changes\n","#                     # get the best in advance\n","#                     best_stats[i] = (best, best_count) \n","#                 if occurances[0][1] > 3: # originally 5\n","#                     print(\"ANSWER FOUND!\")\n","#                     break\n","\n","#             results.append(result_output)\n","#             answers.append(code_output)\n","            \n","#             best_stats[i] = (best, best_count) \n","#             question_type_counts[i] = (text_answers, code_answers)\n","#             total_outputs[i] = outputs\n","            \n","#             total_results[i] = results\n","#             total_answers[i] = answers\n","\n","#             print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n","#             if DEBUG:\n","#                 break\n","                \n","#         print(f\"Predicted best answer: {best_stats}\")\n","#         sample_submission['answer'] = best_stats[i][0]\n","#         correct_answer = test['answer'].values[0]\n","#         current_outputs = total_outputs.get(i, [])\n","#         answers_matched = Counter(current_outputs).get(correct_answer, 0)\n","#         best_matched = best_stats[i][0] == correct_answer # True when most common is correct\n","\n","#         model_score += best_matched * 100 + answers_matched/max(len(current_outputs),1) * 10 \\\n","#             + valid_answers_generated/max(total_answers_to_gen,1) * 10\n","\n","#         # env.predict(sample_submission)\n","#         final_submission = pd.concat([final_submission, sample_submission])\n","\n","#     cur_process = captured.stdout\n","#     cur_process += '\\nprompt correctness:'+ str(prompt_correct_count)\n","#     cur_reflections = ''.join(self_reflection_list)\n","#     all_self_reflections.append(cur_reflections)\n","\n","#     cur_process += '\\n\\n## Self-Reflections\\n'+cur_reflections\n","#     cur_process += '\\n---\\n'\n","\n","#     with open(f'outputs_{i}.txt', 'w') as fh:\n","#         fh.write(cur_process)\n","#     all_captured.append(cur_process)\n","#     total_prompt_correct_count = np.array(total_prompt_correct_count) + np.array(prompt_correct_count)\n","\n","# TIME_SPENT = time.time() - NOTEBOOK_START_TIME - TOTAL_REFLECTION_TIME\n","\n","# model_score = model_score / len(test_df)\n","\n","# import json\n","\n","# print(json.dumps({'model_score': model_score,\n","#                   'prompt_correct': total_prompt_correct_count.tolist(),}))\n","\n","# temperatures = '|'.join(map(str, [temperature, top_p, temperature_coding, top_p_coding]))\n","# folder_name = f'outputs/all_captured_{timestamp}_T={int(TIME_SPENT)}_{n_repetitions}_{TOTAL_TOKENS}_{temperatures}_{model_score:.2f}'\n","# save_current_exp(folder_name, all_captured, final_submission, notebooks=['./main.ipynb', './utils.py'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.702653Z","iopub.status.busy":"2024-05-04T07:00:32.702324Z","iopub.status.idle":"2024-05-04T07:00:32.819166Z","shell.execute_reply":"2024-05-04T07:00:32.818269Z","shell.execute_reply.started":"2024-05-04T07:00:32.702621Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n","\n"]}],"source":["with open('code.py', 'w') as fout:\n","    fout.write(\"print('done')\")\n","\n","batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n","try:\n","    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n","    print(shell_output)\n","except:\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["from utils import *\n","with open('code.py', 'r') as f:\n","    vals = process_code(f.read(), return_shell_output=True)\n","\n","# print(1)\n","print(vals[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-06-22 10:33:21,258] Using an existing study with name 'tuning' instead of creating a new one.\n"]},{"name":"stdout","output_type":"stream","text":["{'temperature': 0.9498695118935835, 'top_p': 0.8652440341753715, 'top_k': 32, 'temperature_coding': 0.9347416308644806, 'top_p_coding': 0.9134585305035123, 'top_k_coding': 48, 'starting_counts': 1}\n","70.14542124542125\n"]}],"source":["import optuna\n","\n","study = optuna.create_study(study_name='tuning', storage='sqlite:///tuning.db', load_if_exists=True)\n","\n","print(study.best_params)\n","print(study.best_value)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4281572,"sourceId":7369493,"sourceType":"datasetVersion"},{"datasetId":4720595,"sourceId":8012825,"sourceType":"datasetVersion"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4748944,"sourceId":8052555,"sourceType":"datasetVersion"},{"modelInstanceId":8332,"sourceId":11261,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":11264,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
