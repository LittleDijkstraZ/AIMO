[32m2024-06-23T18:17:18.628343+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.628343+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.631956+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.631956+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.635379+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.635379+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.635627+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:17:18.635627+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:17:18.640797+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.640797+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.646208+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.646208+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.651313+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.651313+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.651506+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:17:18.651506+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:17:18.659261+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.659261+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.666154+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.666154+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.673677+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.673677+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:17:18.673983+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:17:18.673983+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:18:51.055735+0800[0m [31m[1mAttempt 1: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.055735+0800[0m [31m[1mAttempt 1: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.055735+0800[0m [31m[1mAttempt 1: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.058504+0800[0m [31m[1mAttempt 2: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.058504+0800[0m [31m[1mAttempt 2: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.058504+0800[0m [31m[1mAttempt 2: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.061517+0800[0m [31m[1mAttempt 3: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.061517+0800[0m [31m[1mAttempt 3: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.061517+0800[0m [31m[1mAttempt 3: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.061683+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:18:51.061683+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:18:51.061683+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:18:51.066533+0800[0m [31m[1mAttempt 1: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.066533+0800[0m [31m[1mAttempt 1: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.066533+0800[0m [31m[1mAttempt 1: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.071274+0800[0m [31m[1mAttempt 2: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.071274+0800[0m [31m[1mAttempt 2: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.071274+0800[0m [31m[1mAttempt 2: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.075703+0800[0m [31m[1mAttempt 3: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.075703+0800[0m [31m[1mAttempt 3: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.075703+0800[0m [31m[1mAttempt 3: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.075999+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:18:51.075999+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:18:51.075999+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:18:51.082573+0800[0m [31m[1mAttempt 1: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.082573+0800[0m [31m[1mAttempt 1: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.082573+0800[0m [31m[1mAttempt 1: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.088937+0800[0m [31m[1mAttempt 2: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.088937+0800[0m [31m[1mAttempt 2: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.088937+0800[0m [31m[1mAttempt 2: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.095303+0800[0m [31m[1mAttempt 3: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.095303+0800[0m [31m[1mAttempt 3: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.095303+0800[0m [31m[1mAttempt 3: Error generating response: 'ToTAgent' object has no attribute 'generate'[0m
[32m2024-06-23T18:18:51.096093+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:18:51.096093+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:18:51.096093+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:20:09.920079+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:20:09.923834+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:20:09.926861+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:20:09.927128+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:20:09.932605+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:20:09.937786+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:20:09.942796+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:20:09.943057+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:20:09.950397+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:20:09.957368+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:20:09.964366+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:20:09.964744+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:24:52.317822+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.317822+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.321314+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.321314+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.324445+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.324445+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.324749+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:24:52.324749+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:24:52.329767+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.329767+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.335496+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.335496+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.340922+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.340922+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.341120+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:24:52.341120+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:24:52.348713+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.348713+0800[0m [31m[1mAttempt 1: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.356217+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.356217+0800[0m [31m[1mAttempt 2: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.363415+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.363415+0800[0m [31m[1mAttempt 3: Error generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:24:52.363714+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:24:52.363714+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:25:11.574474+0800[0m [31m[1mAttempt 1: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.574474+0800[0m [31m[1mAttempt 1: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.574474+0800[0m [31m[1mAttempt 1: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.578432+0800[0m [31m[1mAttempt 2: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.578432+0800[0m [31m[1mAttempt 2: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.578432+0800[0m [31m[1mAttempt 2: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.582749+0800[0m [31m[1mAttempt 3: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.582749+0800[0m [31m[1mAttempt 3: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.582749+0800[0m [31m[1mAttempt 3: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.583815+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:25:11.583815+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:25:11.583815+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:25:11.594569+0800[0m [31m[1mAttempt 1: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.594569+0800[0m [31m[1mAttempt 1: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.594569+0800[0m [31m[1mAttempt 1: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.605004+0800[0m [31m[1mAttempt 2: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.605004+0800[0m [31m[1mAttempt 2: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.605004+0800[0m [31m[1mAttempt 2: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.615730+0800[0m [31m[1mAttempt 3: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.615730+0800[0m [31m[1mAttempt 3: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.615730+0800[0m [31m[1mAttempt 3: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.616872+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:25:11.616872+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:25:11.616872+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:25:11.630170+0800[0m [31m[1mAttempt 1: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.630170+0800[0m [31m[1mAttempt 1: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.630170+0800[0m [31m[1mAttempt 1: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.638047+0800[0m [31m[1mAttempt 2: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.638047+0800[0m [31m[1mAttempt 2: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.638047+0800[0m [31m[1mAttempt 2: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.645141+0800[0m [31m[1mAttempt 3: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.645141+0800[0m [31m[1mAttempt 3: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.645141+0800[0m [31m[1mAttempt 3: Error fucking generating response: '>=' not supported between instances of 'int' and 'NoneType'[0m
[32m2024-06-23T18:25:11.645319+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:25:11.645319+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:25:11.645319+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:26:07.319841+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.319841+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.319841+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.319841+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.323308+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.323308+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.323308+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.323308+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.326694+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.326694+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.326694+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.326694+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.327778+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:26:07.327778+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:26:07.327778+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:26:07.327778+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:26:07.333586+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.333586+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.333586+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.333586+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.339661+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.339661+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.339661+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.339661+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.345517+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.345517+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.345517+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.345517+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.345847+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:26:07.345847+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:26:07.345847+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:26:07.345847+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:26:07.353027+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.353027+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.353027+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.353027+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.360773+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.360773+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.360773+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.360773+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.368747+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.368747+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.368747+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.368747+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:26:07.369642+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:26:07.369642+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:26:07.369642+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:26:07.369642+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.750417+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.750417+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.750417+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.750417+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.750417+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.756418+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.756418+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.756418+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.756418+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.756418+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.765253+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.765253+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.765253+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.765253+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.765253+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.766640+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.766640+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.766640+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.766640+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.766640+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.780438+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.780438+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.780438+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.780438+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.780438+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.794674+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.794674+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.794674+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.794674+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.794674+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.806008+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.806008+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.806008+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.806008+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.806008+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.807427+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.807427+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.807427+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.807427+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.807427+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.821844+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.821844+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.821844+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.821844+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.821844+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.829168+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.829168+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.829168+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.829168+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.829168+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.837339+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.837339+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.837339+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.837339+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.837339+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1138, in _validate_generated_length
    if input_ids_length >= generation_config.max_length:
TypeError: '>=' not supported between instances of 'int' and 'NoneType'
[0m
[32m2024-06-23T18:34:34.838689+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.838689+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.838689+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.838689+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:34.838689+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.546290+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.546290+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.546290+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.546290+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.546290+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.546290+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.555873+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.555873+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.555873+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.555873+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.555873+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.555873+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.565479+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.565479+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.565479+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.565479+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.565479+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.565479+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.566757+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.566757+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.566757+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.566757+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.566757+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.566757+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.577225+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.577225+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.577225+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.577225+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.577225+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.577225+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.588659+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.588659+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.588659+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.588659+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.588659+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.588659+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.600218+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.600218+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.600218+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.600218+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.600218+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.600218+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.601437+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.601437+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.601437+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.601437+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.601437+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.601437+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.614941+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.614941+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.614941+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.614941+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.614941+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.614941+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.627656+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.627656+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.627656+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.627656+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.627656+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.627656+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.640884+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.640884+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.640884+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.640884+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.640884+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.640884+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 283, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/1394953768.py", line 258, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:34:40.641970+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.641970+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.641970+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.641970+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.641970+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:34:40.641970+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.311646+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.311646+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.311646+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.311646+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.311646+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.311646+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.311646+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.328358+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.328358+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.328358+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.328358+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.328358+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.328358+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.328358+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.344974+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.344974+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.344974+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.344974+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.344974+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.344974+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.344974+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.346416+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.346416+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.346416+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.346416+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.346416+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.346416+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.346416+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.364677+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.364677+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.364677+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.364677+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.364677+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.364677+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.364677+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.383637+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.383637+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.383637+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.383637+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.383637+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.383637+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.383637+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.571014+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.571014+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.571014+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.571014+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.571014+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.571014+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.571014+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.573647+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.573647+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.573647+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.573647+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.573647+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.573647+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.573647+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.594698+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.594698+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.594698+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.594698+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.594698+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.594698+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.594698+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.616024+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.616024+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.616024+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.616024+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.616024+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.616024+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.616024+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.637011+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.637011+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.637011+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.637011+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.637011+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.637011+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.637011+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_291555/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  File "/tmp/ipykernel_291555/1008666832.py", line 28, in _generate_wrapped
    return org_generate(*x, **y)
  [Previous line repeated 2962 more times]
RecursionError: maximum recursion depth exceeded
[0m
[32m2024-06-23T18:35:20.638549+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.638549+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.638549+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.638549+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.638549+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.638549+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:35:20.638549+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:36:46.352075+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_298187/1789355447.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:36:46.361086+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_298187/1789355447.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:36:46.369515+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_298187/1789355447.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:36:46.369822+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:36:46.380186+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_298187/1789355447.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:36:46.391393+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_298187/1789355447.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:36:46.401864+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_298187/1789355447.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:36:46.402196+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:36:46.414587+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_298187/1789355447.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:36:46.427033+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_298187/1789355447.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:36:46.439564+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_298187/2177328018.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_298187/1789355447.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 972, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
[0m
[32m2024-06-23T18:36:46.439908+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:43:51.671413+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_298187/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_298187/4053575644.py", line 262, in run
    return self.tokenizer.decode(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3825, in decode
    return self._decode(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 625, in _decode
    text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
TypeError: argument 'ids': 'list' object cannot be interpreted as an integer
[0m
[32m2024-06-23T18:43:51.671413+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_298187/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_298187/4053575644.py", line 262, in run
    return self.tokenizer.decode(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3825, in decode
    return self._decode(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 625, in _decode
    text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
TypeError: argument 'ids': 'list' object cannot be interpreted as an integer
[0m
[32m2024-06-23T18:46:26.007149+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_302042/2530192206.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.35 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.32 GiB is free. Process 177644 has 15.32 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 421.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:46:26.116272+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_302042/2530192206.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.35 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.32 GiB is free. Process 177644 has 15.32 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 421.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:46:26.208461+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_302042/2530192206.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.35 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.32 GiB is free. Process 177644 has 15.32 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 421.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:46:26.210027+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:46:26.304143+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_302042/2530192206.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.36 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.32 GiB is free. Process 177644 has 15.32 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 420.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:46:26.392045+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_302042/2530192206.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.36 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.32 GiB is free. Process 177644 has 15.32 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 420.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:46:26.488539+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_302042/2530192206.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.36 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.32 GiB is free. Process 177644 has 15.32 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 420.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:46:26.490101+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:46:26.580066+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_302042/2530192206.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.37 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.32 GiB is free. Process 177644 has 15.32 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 419.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:46:26.668068+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_302042/2530192206.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.37 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.32 GiB is free. Process 177644 has 15.32 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 419.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:46:26.756473+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_302042/2530192206.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.37 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.32 GiB is free. Process 177644 has 15.32 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 419.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:46:26.757773+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:46:26.848220+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_302042/2530192206.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.32 GiB is free. Process 177644 has 15.32 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 418.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:46:26.936122+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_302042/2530192206.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.32 GiB is free. Process 177644 has 15.32 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 418.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:46:27.024394+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_302042/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_302042/2530192206.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.38 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.32 GiB is free. Process 177644 has 15.32 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 418.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:46:27.025676+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:55:22.289255+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_305473/1535753136.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.28 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.65 GiB is free. Process 181778 has 14.99 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 300.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:55:22.358970+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_305473/1535753136.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.28 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.65 GiB is free. Process 181778 has 14.99 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 300.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:55:22.416326+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_305473/1535753136.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.28 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.65 GiB is free. Process 181778 has 14.99 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 300.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:55:22.417384+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:55:22.474002+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_305473/1535753136.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.29 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.65 GiB is free. Process 181778 has 14.99 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 299.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:55:22.531744+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_305473/1535753136.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.29 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.65 GiB is free. Process 181778 has 14.99 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 299.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:55:22.589149+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_305473/1535753136.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.29 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.65 GiB is free. Process 181778 has 14.99 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 299.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:55:22.590431+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:55:22.647120+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_305473/1535753136.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.30 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.65 GiB is free. Process 181778 has 14.99 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 298.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:55:22.704725+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_305473/1535753136.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.30 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.65 GiB is free. Process 181778 has 14.99 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 298.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:55:22.762511+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_305473/1535753136.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.30 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.65 GiB is free. Process 181778 has 14.99 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 298.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:55:22.763681+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
[32m2024-06-23T18:55:22.820201+0800[0m [31m[1mAttempt 1: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_305473/1535753136.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.31 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.65 GiB is free. Process 181778 has 14.99 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 297.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:55:22.876740+0800[0m [31m[1mAttempt 2: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_305473/1535753136.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.31 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.65 GiB is free. Process 181778 has 14.99 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 297.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:55:22.934456+0800[0m [31m[1mAttempt 3: Error fucking generating response: Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/swarms/structs/agent.py", line 814, in run
    response = self.llm(*response_args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 279, in __call__
    return self.run(task, *args, **kwargs)
  File "/tmp/ipykernel_305473/4053575644.py", line 254, in run
    outputs = self.model.generate(
  File "/tmp/ipykernel_305473/1535753136.py", line 23, in _generate_wrapped
    return org_generate(*x, **y)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 1575, in generate
    result = self._sample(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2697, in _sample
    outputs = self(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1196, in forward
    outputs = self.model(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1016, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 739, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 670, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.31 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.65 GiB is free. Process 181778 has 14.99 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 297.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[0m
[32m2024-06-23T18:55:22.935428+0800[0m [31m[1mFailed to generate a valid response after retry attempts.[0m
