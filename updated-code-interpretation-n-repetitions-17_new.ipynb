{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Forked from https://www.kaggle.com/code/abdurrafae/improved-code-interpretation"]},{"cell_type":"markdown","metadata":{},"source":["**Lewis:** the only changes in this notebook are those needed to run the original one with the new Kaggle evaluation API"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:30.899592Z","iopub.status.busy":"2024-06-17T14:14:30.899345Z","iopub.status.idle":"2024-06-17T14:14:31.357778Z","shell.execute_reply":"2024-06-17T14:14:31.356805Z","shell.execute_reply.started":"2024-06-17T14:14:30.899570Z"},"trusted":true},"outputs":[],"source":["import time\n","import torch\n","import aimo\n","\n","\n","NOTEBOOK_START_TIME = time.time()\n","\n","\n","DEBUG = False\n","\n","QUANT = False\n","\n","if QUANT:\n","    from transformers import BitsAndBytesConfig\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit = True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_use_double_quant=True,\n","    )\n","\n","USE_PAST_KEY = True\n","\n","\n","import gc\n","'''\n","accelerate execution a bit\n","'''\n","torch.backends.cuda.enable_mem_efficient_sdp(False)\n","\n","from transformers import (\n","    AutoModelForCausalLM, \n","    AutoTokenizer, \n","    AutoConfig,\n","    StoppingCriteria,\n","    set_seed\n",")\n","\n","import transformers\n","print(f\"Transformers Version: {transformers.__version__}\")\n","set_seed(42)\n","\n","import pandas as pd\n","from tqdm import tqdm\n","PRIVATE = True\n","\n","# df = pd.read_csv('./input/ai-mathematical-olympiad-prize/test.csv')\n","# df.head()\n","\n","# PRIVATE = True\n","df = pd.read_csv('./input/ai-mathematical-olympiad-prize/train.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["TO-DO\n","\n","Change temperature as the question goes longer\n","Change temperature based on question lenght"]},{"cell_type":"markdown","metadata":{},"source":["# Zero-shot MMOS-DeepSeekMath-7B with self-consistency and generated code reasoning evaluation\n","\n","Self-consistency is a modification of the standard greedy decoding in reasoning pipelines via sampling several diverse answers followed by aggregation, e.g., most common answer ([SC-CoT paper](https://arxiv.org/pdf/2203.11171.pdf)).\n","\n","In this kernel, we will consider MMOS-DeepSeekMath-7B RL-tuned backbone; in my experiments, this model produces more consistent code reasoning and the code block execution will allow us to decrease arithmetic hallucinations."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:14:59.705246Z","iopub.status.busy":"2024-06-17T14:14:59.704540Z","iopub.status.idle":"2024-06-17T14:14:59.709463Z","shell.execute_reply":"2024-06-17T14:14:59.708511Z","shell.execute_reply.started":"2024-06-17T14:14:59.705213Z"},"papermill":{"duration":1.224774,"end_time":"2024-02-29T09:36:31.21757","exception":false,"start_time":"2024-02-29T09:36:29.992796","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from .utils import *"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:19:17.265368Z","iopub.status.busy":"2024-06-17T14:19:17.264623Z","iopub.status.idle":"2024-06-17T14:19:17.271565Z","shell.execute_reply":"2024-06-17T14:19:17.270717Z","shell.execute_reply.started":"2024-06-17T14:19:17.265336Z"},"trusted":true},"outputs":[],"source":["def naive_parse(answer):\n","    '''\n","    get the first consecutive numbers\n","    '''\n","    out = []\n","    start = False\n","    end = False\n","    for l in reversed(list(answer)):\n","        if l in '0123456789' and not end:\n","            start = True\n","            out.append(l)\n","        else:\n","            if start:\n","                end = True\n","        \n","    out = reversed(out)\n","    return ''.join(out)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T16:57:03.979978Z","iopub.status.busy":"2024-05-05T16:57:03.9797Z","iopub.status.idle":"2024-05-05T16:57:04.220476Z","shell.execute_reply":"2024-05-05T16:57:04.219456Z","shell.execute_reply.started":"2024-05-05T16:57:03.97995Z"},"trusted":true},"outputs":[{"data":{"text/plain":["20"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-05T16:57:04.223341Z","iopub.status.busy":"2024-05-05T16:57:04.221614Z","iopub.status.idle":"2024-05-05T16:57:04.229038Z","shell.execute_reply":"2024-05-05T16:57:04.22822Z","shell.execute_reply.started":"2024-05-05T16:57:04.223316Z"},"trusted":true},"outputs":[],"source":["import re\n","import math\n","import random\n","\n","from collections import defaultdict\n","\n","# n_repetitions = 12 if PRIVATE else 4 # Original notebook had 22 but times out :(\n","n_repetitions = 8 if PRIVATE else 4 # Original notebook had 22 but times out :(\n","\n","TOTAL_TOKENS = 2048 # if PRIVATE else 512\n","# TOTAL_TOKENS = 512 # if PRIVATE else 512\n","TIME_LIMIT = 31500\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T06:53:23.248311Z","iopub.status.busy":"2024-05-04T06:53:23.248014Z","iopub.status.idle":"2024-05-04T06:56:25.02544Z","shell.execute_reply":"2024-05-04T06:56:25.024594Z","shell.execute_reply.started":"2024-05-04T06:53:23.248286Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9713d939a3054efe8441193737b9172a","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#list number of cuda\n","n_gpus = torch.cuda.device_count()\n","device_i = 0\n","if n_gpus > 1:\n","    device_i = 1\n","\n","\n","\n","if PRIVATE:\n","\n","    MODEL_PATH = \"./input/deepseek-math\"#\"/kaggle/input/gemma/transformers/7b-it/1\"\n","    DEEP = True\n","\n","    config = AutoConfig.from_pretrained(MODEL_PATH)\n","    config.gradient_checkpointing = True\n","\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","\n","    device_map = [('model.embed_tokens', 0),\n","                 ('model.layers.0', 0),\n","                 ('model.layers.1', 0),\n","                 ('model.layers.2', 0),\n","                 ('model.layers.3', 0),\n","                 ('model.layers.4', 0),\n","                 ('model.layers.5', 0),\n","                 ('model.layers.6', 0),\n","                 ('model.layers.7', 0),\n","                 ('model.layers.8', 0),\n","                 ('model.layers.9', 0),\n","                 ('model.layers.10', 0),\n","                 ('model.layers.11', 0),\n","                 ('model.layers.12', 0),\n","                 ('model.layers.13', 0),\n","                 ('model.layers.14', 0),\n","                 ('model.layers.15', 0),\n","                 ('model.layers.16', 0),\n","                 ('model.layers.17', 0),\n","                 ('model.layers.18', 0),\n","                 ('model.layers.19', 0),\n","                 ('model.layers.20', 0),\n","                 ('model.layers.21', 0),\n","                 ('model.layers.22', device_i),\n","                 ('model.layers.23', device_i),\n","                 ('model.layers.24', device_i),\n","                 ('model.layers.25', device_i),\n","                 ('model.layers.26', device_i),\n","                 ('model.layers.27', device_i),\n","                 ('model.layers.28', device_i),\n","                 ('model.layers.29', device_i),\n","                 ('model.norm', device_i),\n","                 ('lm_head', device_i)]\n","\n","    device_map = {ii:jj for (ii,jj) in device_map}\n","\n","    if QUANT:\n","        from transformers import BitsAndBytesConfig\n","        quantization_config = BitsAndBytesConfig(\n","            load_in_4bit = True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.bfloat16,\n","            bnb_4bit_use_double_quant=True,\n","        )\n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_PATH,\n","            device_map=\"sequential\",\n","            torch_dtype=\"auto\",\n","            trust_remote_code=True, \n","            quantization_config=quantization_config,\n","            config=config\n","        )\n","    else:  \n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_PATH,\n","            device_map=device_map,\n","            torch_dtype=\"auto\",\n","            trust_remote_code=True,\n","            #quantization_config=quantization_config,\n","            config=config\n","        )\n","    \n","    pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype='auto',\n","    device_map=device_map,\n","    )\n","    \n","    from transformers import StoppingCriteriaList\n","\n","    class StoppingCriteriaSub(StoppingCriteria):\n","        def __init__(self, stops = [], encounters=1):\n","            super().__init__()\n","            self.stops = [stop.to(\"cuda\") for stop in stops]\n","\n","        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n","            for stop in self.stops:\n","                last_token = input_ids[0][-len(stop):]\n","                if torch.all(torch.eq(stop,last_token)):\n","                    return True\n","            return False\n","\n","\n","    stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"] #,  \n","    stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n","    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n","    \n","    model.dtype, model.hf_device_map"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T06:56:25.029669Z","iopub.status.busy":"2024-05-04T06:56:25.029303Z","iopub.status.idle":"2024-05-04T06:56:25.035377Z","shell.execute_reply":"2024-05-04T06:56:25.034298Z","shell.execute_reply.started":"2024-05-04T06:56:25.029643Z"},"trusted":true},"outputs":[],"source":["\n","code = \\\n","\"\"\"Below is a math problem you are to solve (positive numerical answer):\n","\\\"{}\\\"\n","To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\n","Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n","\n","Approach:\"\"\"\n","\n","\n","# cot = \\\n","# \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n","# \\\"{}\\\"\n","# Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n","\n","cot = \\\n","\"\"\"Below is a math problem you are to solve (positive numerical answer!):\n","\\\"{}\\\"\n","Analyze this problem and think step by step to come to a solution. You may use python to assist with solving it. Output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n","\n","\n","#     code = \\\n","# \"\"\"\n","# 假设你是IMO数学竞赛指导老师，以下是一个数学问题：\n","# \\\"{}\\\"\n","# \\n\n","# 1. 请先把这个问题翻译成中文。\n","# 2. 接着，确定一个基于sympy，numpy，或者scipy的解题方法，列出每一步的操作。\n","# 3. 请编对应的python脚本，并打印结果。\n","# 4. 你的最终答案应该是正整数，而不是代数表达式！\n","# 5. 解决问题后，在\\\\boxed{}中输出最终的数值答案。\n","\n","# 解：\n","# \"\"\"\n","#     cot = \\\n","# \"\"\"\n","# 假设你是IMO数学竞赛指导老师，以下是一个数学问题：\n","# \\\"{}\\\"\n","# \\n\n","# 1. 请先把这个问题翻译成中文。\n","# 2. 分析这个问题，明确问题的要求与条件，并确定解题思路。\n","# 3. 尝试一步步解决这个问题，在必要的地方列式计算。 \n","# 4. 必要时，可以通过python脚本来辅助计算。\n","# 5. 解决问题后，在\\\\boxed{}中输出最终的数值答案。\n","\n","# 解：\n","# \"\"\"\n","\n","\n","prompt_options = [code,cot]\n","# prompt_options = [code]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-05-04T06:56:25.037129Z","iopub.status.busy":"2024-05-04T06:56:25.036853Z","iopub.status.idle":"2024-05-04T07:00:32.65925Z","shell.execute_reply":"2024-05-04T07:00:32.658351Z","shell.execute_reply.started":"2024-05-04T06:56:25.037105Z"},"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import re\n","from collections import defaultdict\n","from collections import Counter\n","\n","import json\n","\n","import shutil\n","import os\n","from IPython.display import display, Javascript\n","\n","from numpy.random import choice\n","import numpy as np\n","from IPython.utils import io\n","from tqdm.auto import tqdm\n","\n","# tool_instruction = '\\n\\nPlease integrate natural language reasoning with programs to solve the above problem, and put your final numerical answer within \\\\boxed{}.\\nNote that the intermediary calculations may be real numbers, but the final numercal answer would always be an integer.'\n","\n","\n","# temperature = 2.0\n","temperature = 0.85\n","top_p = 1.0\n","\n","temperature_coding = 0.85 # code need to be strict, but still should have some creativity to get through difficult problems\n","top_p_coding = 1.0\n","\n","   \n","total_results = {}\n","total_answers = {}\n","best_stats = {}\n","total_outputs = {}\n","question_type_counts = {}\n","starting_counts = (2,3)\n","\n","\n","\n","all_captured = []\n","\n","import datetime\n","timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","\n","all_answers = {}\n","\n","test_df = df.read_csv('./input/ai-mathematical-olympiad-prize/test.csv')\n","submission_df = df.read_csv('./input/ai-mathematical-olympiad-prize/sample_submission.csv')\n","\n","\n","\n","final_submission = pd.DataFrame(columns=submission_df.columns)\n","\n","model_score = 0\n","\n","for i in range(len(test_df)):\n","    test, sample_submission = test_df.loc[i:i, :], submission_df.loc[i:i, :]\n","    # iterate through every problems in the environment\n","    # use a loop get the test row and the corresponding sample_submissions row\n","\n","    total_answers_to_gen = 0\n","    valid_answers_generated = 0\n","    answers_matched = 0\n","    with io.capture_output() as captured: # i capture the printouts\n","\n","        print(f\"Solving problem {i} ...\")\n","        TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n","\n","        if TIME_SPENT>TIME_LIMIT: # submit the prediction if no time is left\n","            sample_submission['answer'] = 0\n","            env.predict(sample_submission)\n","            break\n","            \n","        for trial_j in tqdm(range(n_repetitions)):   \n","\n","            problem = test['problem'].values[0]\n","            print(f\"\\n\\n\\nQUESTION {i} - {trial_j} - TIME_SPENT : {TIME_SPENT:.0f} secs\")\n","            \n","            best, best_count = best_stats.get(i,(-1,-1))\n","            if best_count>np.sqrt(trial_j)+1:\n","                print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n","                continue\n","\n","            total_answers_to_gen += 1\n","                \n","            outputs = total_outputs.get(i,[])\n","            text_answers, code_answers = question_type_counts.get(i,starting_counts)\n","            results = total_results.get(i,[])\n","            answers = total_answers.get(i,[])\n","            \n","            for _ in range(5): # clean the cache\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                time.sleep(0.2)\n","\n","            try:\n","                ALREADY_GEN = 0\n","                code_error = None\n","                code_error_count = 0\n","                code_output = -1\n","                #initail_message = problem  + tool_instruction \n","                counts = np.array([text_answers,code_answers])\n","\n","                draw = choice(prompt_options, 1,\n","                            p=counts/counts.sum())\n","\n","                initail_message = draw[0].format(problem,\"{}\")            \n","                prompt = f\"User: {initail_message}\"\n","\n","                current_printed = len(prompt)\n","                print(f\"{trial_j}_{prompt}\\n\")\n","\n","                model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","                input_len = len(model_inputs['input_ids'][0])\n","\n","                generation_output = model.generate(**model_inputs, \n","                                                max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n","                                                return_dict_in_generate=USE_PAST_KEY,\n","                                                do_sample = True,\n","                                                temperature = temperature,\n","                                                top_p = top_p,\n","                                                num_return_sequences=1,\n","                                                stopping_criteria = stopping_criteria)\n","\n","                if USE_PAST_KEY:\n","                    output_ids = generation_output.sequences[0]\n","                else:\n","                    output_ids = generation_output[0]\n","                decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","                print(f\"{decoded_output[current_printed:]}\\n\")\n","                current_printed += len(decoded_output[current_printed:])\n","                cummulative_code = \"\"\n","                \n","                \n","                stop_word_cond = False\n","                for stop_word in stop_words:\n","                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","                    \n","                \n","                while (stop_word_cond) and (ALREADY_GEN<(TOTAL_TOKENS)):\n","\n","                    if (decoded_output[-len(\"```python\"):]==\"```python\"):\n","                        temperature_inner=temperature_coding\n","                        top_p_inner = top_p_coding\n","                        prompt = decoded_output\n","                    else:\n","                        temperature_inner=temperature\n","                        top_p_inner = top_p\n","                        try:\n","                            if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n","                                code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n","                            else:\n","                                code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n","                            \n","\n","                            cummulative_code+=code_text\n","                            code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n","                            print('CODE RESULTS', code_output)\n","                            \n","                            # TODO: try to add the following prompt:\n","                            # Senario: CODE RESULTS 888.888888888889\n","                            # The result is not a positive integer, so we must have made a mistake somewhere. Let's go back and check our work.\n","\n","\n","                            if code_error==code_output:\n","                                code_error_count+=1\n","                            else:\n","                                code_error=code_output\n","                                code_error_count = 0\n","\n","                            if not CODE_STATUS:\n","                                cummulative_code = cummulative_code[:-len(code_text)]\n","\n","                                if code_error_count>=1:\n","                                    print(\"REPEATED ERRORS\")\n","                                    break\n","\n","                        except Exception as e:\n","                            print(e)\n","                            print('ERROR PARSING CODE')\n","                            code_output = -1\n","\n","                       \n","                        if code_output!=-1:\n","                            self_correcting_prompt = ''\n","                        \n","                            if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n","                                prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'+self_correcting_prompt\n","                            else:\n","                                prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n' +self_correcting_prompt\n","                        else:\n","                            self_correcting_prompt = \"Let's analyze and fix the error in our code, and then continue to solve the math problem.\"\n","\n","                            prompt = decoded_output\n","                            cummulative_code=\"\"\n","\n","\n","\n","                    model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","                    ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n","\n","                    if USE_PAST_KEY:\n","                        old_values = generation_output.past_key_values\n","                    else:\n","                        old_values = None\n","\n","                    generation_output = model.generate(**model_inputs, \n","                                                    max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n","                                                    return_dict_in_generate=USE_PAST_KEY,\n","                                                    past_key_values=old_values,\n","                                                    do_sample = True,\n","                                                    temperature = temperature_inner,\n","                                                    top_p = top_p_inner,\n","                                                    num_return_sequences=1, \n","                                                    stopping_criteria = stopping_criteria)\n","\n","                    if USE_PAST_KEY:\n","                        output_ids = generation_output.sequences[0]\n","                    else:\n","                        output_ids = generation_output[0]\n","                    decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","                    print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n","                    current_printed+=len(decoded_output[current_printed:])\n","                    \n","                    stop_word_cond = False\n","                    for stop_word in stop_words:\n","                        stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","\n","                if USE_PAST_KEY:\n","                    output_ids = generation_output.sequences[0]\n","                else:\n","                    output_ids = generation_output[0]\n","\n","                raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n","                #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n","                result_output = process_text_output(raw_output)\n","                \n","                try:\n","                    code_output = round(float(eval(code_output))) % 1000\n","                except Exception as e:\n","                    print(e,'final_eval')\n","                    code_output = -1\n","\n","            except Exception as e:\n","                print(e,\"5\")\n","                result_output, code_output = -1, -1\n","\n","            valid_answer_obtained = False\n","            if code_output!=-1:\n","                outputs.append(code_output)\n","                code_answers+=1\n","                valid_answer_obtained = True\n","\n","            if result_output!=-1:\n","                outputs.append(result_output)\n","                text_answers+=1\n","                valid_answer_obtained = True\n","\n","            valid_answers_generated += int(valid_answer_obtained)\n","\n","            if len(outputs) > 0:\n","                occurances = Counter(outputs).most_common()\n","                print(occurances)\n","                if occurances[0][1] > best_count:\n","                    print(\"GOOD ANSWER UPDATED!\")\n","                    best = occurances[0][0]\n","                    best_count = occurances[0][1]\n","                    # in case the following break happens while the best changes\n","                    # get the best in advance\n","                    best_stats[i] = (best, best_count) \n","                if occurances[0][1] > 3: # originally 5\n","                    print(\"ANSWER FOUND!\")\n","                    break\n","\n","            results.append(result_output)\n","            answers.append(code_output)\n","            \n","            best_stats[i] = (best, best_count) \n","            question_type_counts[i] = (text_answers, code_answers)\n","            total_outputs[i] = outputs\n","            \n","            total_results[i] = results\n","            total_answers[i] = answers\n","\n","            print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n","            if DEBUG:\n","                break\n","                \n","        print(f\"Predicted best answer: {best_stats}\")\n","        sample_submission['answer'] = best_stats[i][0]\n","        correct_answer = test['answer'].values[0]\n","        current_outputs = total_outputs.get(i, [])\n","        answers_matched = Counter(current_outputs).get(correct_answer, 0)\n","        best_matched = best_stats[i][0] == correct_answer # True when most common is correct\n","\n","        model_score += best_matched * 100 + answers_matched/len(current_outputs) * 10 \\\n","            + valid_answers_generated/total_answers_to_gen * 10\n","\n","        # env.predict(sample_submission)\n","        final_submission = pd.concat([final_submission, sample_submission])\n","\n","    cur_process = captured.stdout\n","    cur_process += '\\n==sep==\\n'\n","    with open(f'outputs_{i}.txt', 'w') as fh:\n","        fh.write(cur_process)\n","    all_captured.append(cur_process)\n","\n","model_score = model_score / len(test_df)\n","\n","\n","print(json.dumps({'model_score': model_score}))\n","\n","temperatures = '|'.join(map(str, [temperature, top_p, temperature_coding, top_p_coding]))\n","folder_name = f'all_captured_{timestamp}_T={int(TIME_SPENT)}_{n_repetitions}_{TOTAL_TOKENS}_{QUANT}_{temperatures}'\n","\n","\n","\n","def save_current_exp(folder_name, all_captured):\n","    import os\n","    os.makedirs(folder_name, exist_ok=True)\n","    for i, captured in enumerate(all_captured):\n","        with open(f'{folder_name}/output_{i}.txt', 'w') as fh:\n","            fh.write(captured)\n","        # change to md\n","        os.rename(f'{folder_name}/output_{i}.txt', f'{folder_name}/output_{i}.md')\n","\n","    all_captured_txt = '\\n'.join(all_captured)\n","    with open(f'{folder_name}/all_outputs.txt', 'w') as fh:\n","        fh.write(all_captured_txt)\n","        \n","    final_submission = final_submission.reset_index(drop=True)\n","    final_submission.to_csv(f'{folder_name}/submission.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","from collections import defaultdict\n","from collections import Counter\n","\n","from numpy.random import choice\n","import numpy as np\n","from IPython.utils import io\n","from tqdm.auto import tqdm\n","\n","# tool_instruction = '\\n\\nPlease integrate natural language reasoning with programs to solve the above problem, and put your final numerical answer within \\\\boxed{}.\\nNote that the intermediary calculations may be real numbers, but the final numercal answer would always be an integer.'\n","\n","\n","# temperature = 2.0\n","temperature = 0.85\n","# top_p = 1.0\n","top_p =  1\n","top_k = 40\n","\n","\n","temperature_coding =  0.9 # code need to be strict, but still should have some creativity to get through difficult problems\n","top_p_coding = 0.8\n","top_k_coding = 40\n","\n","   \n","total_results = {}\n","total_answers = {}\n","best_stats = {}\n","total_outputs = {}\n","question_type_counts = {}\n","starting_counts = (2,3)\n","\n","all_captured = []\n","\n","import datetime\n","timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","\n","all_answers = {}\n","\n","for i, (test, sample_submission) in tqdm(enumerate(iter_test), total=10):\n","    # iterate through every problems in the environment\n","    # use a loop get the test row and the corresponding sample_submissions row\n","\n","    with io.capture_output() as captured: # i capture the printouts\n","\n","        print(f\"Solving problem {i} ...\")\n","        TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n","\n","        if TIME_SPENT>TIME_LIMIT: # submit the prediction if no time is left\n","            sample_submission['answer'] = 0\n","            env.predict(sample_submission)\n","            break\n","            \n","        for trial_j in tqdm(range(n_repetitions)):   \n","\n","            problem = test['problem'].values[0]\n","            print(f\"\\n\\n\\nQUESTION {i} - {trial_j} - TIME_SPENT : {TIME_SPENT:.0f} secs\")\n","            \n","            best, best_count = best_stats.get(i,(-1,-1))\n","            if best_count>np.sqrt(trial_j)+1:\n","                print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n","                continue\n","                \n","            outputs = total_outputs.get(i,[])\n","            text_answers, code_answers = question_type_counts.get(i,starting_counts)\n","            results = total_results.get(i,[])\n","            answers = total_answers.get(i,[])\n","            \n","            for _ in range(5): # clean the cache\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                time.sleep(0.2)\n","\n","            try:\n","                ALREADY_GEN = 0\n","                code_error = None\n","                code_error_count = 0\n","                code_output = -1\n","\n","                counts = np.array([text_answers, code_answers])\n","\n","                if trial_j<len(prompt_options):\n","                    draw = [prompt_options[trial_j%len(prompt_options)]] # alternating \n","                else:\n","                    draw = choice(prompt_options, 1,\n","                                p=counts/counts.sum()) # chose prompts adaptively\n","\n","                initail_message = draw[0].format(problem,\"{}\")            \n","                prompt = f\"User: {initail_message}\"\n","\n","                current_printed = len(prompt)\n","                print(f\"{trial_j}_{prompt}\\n\")\n","\n","                model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","                input_len = len(model_inputs['input_ids'][0])\n","\n","                generation_output = model.generate(**model_inputs, \n","                                                max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n","                                                return_dict_in_generate=USE_PAST_KEY,\n","                                                do_sample = True,\n","                                                temperature = temperature,\n","                                                top_p = top_p,\n","                                                top_k = top_k,\n","                                                num_return_sequences=1,\n","                                                stopping_criteria = stopping_criteria)\n","\n","                if USE_PAST_KEY:\n","                    output_ids = generation_output.sequences[0]\n","                else:\n","                    output_ids = generation_output[0]\n","                decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","                print(f\"{decoded_output[current_printed:]}\\n\")\n","                current_printed += len(decoded_output[current_printed:])\n","                cummulative_code = \"\"\n","                \n","                \n","                stop_word_cond = False\n","                for stop_word in stop_words:\n","                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","                    \n","                \n","                while (stop_word_cond) and (ALREADY_GEN<(TOTAL_TOKENS)):\n","\n","                    if (decoded_output[-len(\"```python\"):]==\"```python\"):\n","                        temperature_inner=temperature_coding\n","                        top_p_inner = top_p_coding\n","                        top_k_inner = top_k_coding\n","                        prompt = decoded_output\n","                    else:\n","                        temperature_inner=temperature\n","                        top_p_inner = top_p\n","                        top_k_inner = top_k\n","                        try:\n","                            if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n","                                code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n","                            else:\n","                                code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n","                            \n","\n","                            cummulative_code+=code_text\n","                            code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n","                            print('CODE RESULTS', code_output)\n","                            \n","                            # TODO: try to add the following prompt:\n","                            # Senario: CODE RESULTS 888.888888888889\n","                            # The result is not a positive integer, so we must have made a mistake somewhere. Let's go back and check our work.\n","\n","\n","                            if code_error==code_output:\n","                                code_error_count+=1\n","                            else:\n","                                code_error=code_output\n","                                code_error_count = 0\n","\n","                            if not CODE_STATUS:\n","                                cummulative_code = cummulative_code[:-len(code_text)]\n","\n","                                if code_error_count>=1:\n","                                    print(\"REPEATED ERRORS\")\n","                                    break\n","\n","                        except Exception as e:\n","                            print(e)\n","                            print('ERROR PARSING CODE')\n","                            code_output = -1\n","\n","                       \n","\n","                        if code_output!=-1:\n","                            self_correcting_prompt = ''\n","                            if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n","                                prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'+self_correcting_prompt\n","                            else:\n","                                prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n' +self_correcting_prompt\n","                        else:\n","                            break # dont fix it lol, it's useless\n","                            prompt = decoded_output + self_correcting_prompt\n","                            cummulative_code=\"\"\n","\n","\n","\n","                    model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","                    ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n","\n","                    if USE_PAST_KEY:\n","                        old_values = generation_output.past_key_values\n","                    else:\n","                        old_values = None\n","\n","                    generation_output = model.generate(**model_inputs, \n","                                                    max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n","                                                    return_dict_in_generate=USE_PAST_KEY,\n","                                                    past_key_values=old_values,\n","                                                    do_sample = True,\n","                                                    temperature = temperature_inner,\n","                                                    top_p = top_p_inner,\n","                                                    top_k = top_k_inner,\n","                                                    num_return_sequences=1, \n","                                                    stopping_criteria = stopping_criteria)\n","\n","                    if USE_PAST_KEY:\n","                        output_ids = generation_output.sequences[0]\n","                    else:\n","                        output_ids = generation_output[0]\n","                    decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","                    print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n","                    current_printed+=len(decoded_output[current_printed:])\n","                    \n","                    stop_word_cond = False\n","                    for stop_word in stop_words:\n","                        stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n","\n","                if USE_PAST_KEY:\n","                    output_ids = generation_output.sequences[0]\n","                else:\n","                    output_ids = generation_output[0]\n","\n","                raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n","                #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n","                result_output = process_text_output(raw_output)\n","                \n","                try:\n","                    code_output = round(float(eval(code_output))) % 1000\n","                except Exception as e:\n","                    print(e,'final_eval')\n","                    code_output = -1\n","\n","            except Exception as e:\n","                print(e,\"5\")\n","                result_output, code_output = -1, -1\n","\n","            if code_output!=-1:\n","                outputs.append(code_output)\n","                code_answers+=1\n","\n","            if result_output!=-1:\n","                outputs.append(result_output)\n","                text_answers+=1\n","\n","            if len(outputs) > 0:\n","                occurances = Counter(outputs).most_common()\n","                print(occurances)\n","                if occurances[0][1] > best_count:\n","                    print(\"GOOD ANSWER UPDATED!\")\n","                    best = occurances[0][0]\n","                    best_count = occurances[0][1]\n","                if occurances[0][1] > 5:\n","                    print(\"ANSWER FOUND!\")\n","                    break\n","\n","            results.append(result_output)\n","            answers.append(code_output)\n","            \n","            best_stats[i] = (best, best_count) \n","            question_type_counts[i] = (text_answers, code_answers)\n","            total_outputs[i] = outputs\n","            \n","            total_results[i] = results\n","            total_answers[i] = answers\n","\n","            print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n","            if DEBUG:\n","                break\n","                \n","        print(f\"Predicted best answer: {best_stats}\")\n","        sample_submission['answer'] = best_stats[i][0]\n","        env.predict(sample_submission)\n","\n","    cur_process = captured.stdout\n","    cur_process += '\\n==sep==\\n'\n","    with open(f'outputs_{i}.txt', 'w') as fh:\n","        fh.write(cur_process)\n","    all_captured.append(cur_process)\n","\n","\n","temperatures = '|'.join(map(str, [temperature, top_p, temperature_coding, top_p_coding]))\n","folder_name = f'all_captured_{timestamp}_T={int(TIME_SPENT)}_{n_repetitions}_{TOTAL_TOKENS}_{QUANT}_{temperatures}'\n","\n","import os\n","os.makedirs(folder_name, exist_ok=True)\n","for i, captured in enumerate(all_captured):\n","    with open(f'{folder_name}/output_{i}.txt', 'w') as fh:\n","        fh.write(captured)\n","    # change to md\n","    os.rename(f'{folder_name}/output_{i}.txt', f'{folder_name}/output_{i}.md')\n","\n","all_captured_txt = '\\n'.join(all_captured)\n","with open(f'{folder_name}/all_outputs.txt', 'w') as fh:\n","    fh.write(all_captured_txt)\n","\n","\n","import shutil\n","import os\n","from IPython.display import display, Javascript\n","\n","def save_and_copy_notebook(source_path, destination_path):\n","    # Save the notebook first\n","    display(Javascript('IPython.notebook.save_checkpoint();'))\n","    \n","    # Ensure the notebook is saved before copying\n","    import time\n","    time.sleep(3)  # Wait for a few seconds before copying\n","\n","    # Copy the notebook\n","    shutil.copy2(source_path, destination_path)\n","    print(f\"Notebook copied to {destination_path}\")\n","\n","# Usage example: specify the current notebook path and the target location\n","current_notebook_path = f'./updated-code-interpretation-n-repetitions-17.ipynb'\n","new_location_path = f'{folder_name}/notebook.ipynb'\n","save_and_copy_notebook(current_notebook_path, new_location_path)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["\n","# import os\n","# os.makedirs(folder_name, exist_ok=True)\n","# for i, captured in enumerate(all_captured):\n","#     with open(f'{folder_name}/output_{i}.txt', 'w') as fh:\n","#         fh.write(captured)\n","#     # change to md\n","#     os.rename(f'{folder_name}/output_{i}.txt', f'{folder_name}/output_{i}.md')\n","\n","# all_captured = '\\n'.join(all_captured)\n","# with open(f'{folder_name}/all_outputs.txt', 'w') as fh:\n","#     fh.write(all_captured)\n","\n","\n","# import shutil\n","# import os\n","\n","# from IPython.display import display, Javascript\n","\n","# def save_and_copy_notebook(source_path, destination_path):\n","#     # Save the notebook first\n","#     display(Javascript('IPython.notebook.save_checkpoint();'))\n","    \n","#     # Ensure the notebook is saved before copying\n","#     import time\n","#     time.sleep(3)  # Wait for a few seconds before copying\n","\n","#     # Copy the notebook\n","#     shutil.copy2(source_path, destination_path)\n","#     print(f\"Notebook copied to {destination_path}\")\n","\n","# # Usage example: specify the current notebook path and the target location\n","# current_notebook_path = f'./updated-code-interpretation-n-repetitions-17.ipynb'\n","# new_location_path = f'{folder_name}/notebook.ipynb'\n","# save_and_copy_notebook(current_notebook_path, new_location_path)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.666603Z","iopub.status.busy":"2024-05-04T07:00:32.66628Z","iopub.status.idle":"2024-05-04T07:00:32.680081Z","shell.execute_reply":"2024-05-04T07:00:32.6793Z","shell.execute_reply.started":"2024-05-04T07:00:32.666572Z"},"trusted":true},"outputs":[],"source":["# if PRIVATE:\n","#     df['answer'] = [best_stats[ii][0] for ii in range(len(df))]\n","# else:\n","#     df['answer'] = 2"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.681659Z","iopub.status.busy":"2024-05-04T07:00:32.681425Z","iopub.status.idle":"2024-05-04T07:00:32.690243Z","shell.execute_reply":"2024-05-04T07:00:32.689341Z","shell.execute_reply.started":"2024-05-04T07:00:32.681638Z"},"papermill":{"duration":0.021128,"end_time":"2024-02-29T09:37:05.574782","exception":false,"start_time":"2024-02-29T09:37:05.553654","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# df[['id','answer']].to_csv(\"foo.csv\", header=True, index=False)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.692931Z","iopub.status.busy":"2024-05-04T07:00:32.6913Z","iopub.status.idle":"2024-05-04T07:00:32.701164Z","shell.execute_reply":"2024-05-04T07:00:32.700328Z","shell.execute_reply.started":"2024-05-04T07:00:32.692898Z"},"trusted":true},"outputs":[],"source":["# if not PRIVATE:\n","#     df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n","#     if PRIVATE:\n","#         df['model_answer'] = [best_stats[ii][0] for ii in range(len(df))]\n","#         df['match'] = df.answer == df.model_answer\n","#         print(f'{df.match.sum()} matches in {len(df)} examples')"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T07:00:32.702653Z","iopub.status.busy":"2024-05-04T07:00:32.702324Z","iopub.status.idle":"2024-05-04T07:00:32.819166Z","shell.execute_reply":"2024-05-04T07:00:32.818269Z","shell.execute_reply.started":"2024-05-04T07:00:32.702621Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n","\n"]}],"source":["with open('code.py', 'w') as fout:\n","    fout.write(\"print('done')\")\n","\n","batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n","try:\n","    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n","    print(shell_output)\n","except:\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4281572,"sourceId":7369493,"sourceType":"datasetVersion"},{"datasetId":4720595,"sourceId":8012825,"sourceType":"datasetVersion"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4748944,"sourceId":8052555,"sourceType":"datasetVersion"},{"modelInstanceId":8332,"sourceId":11261,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":11264,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
